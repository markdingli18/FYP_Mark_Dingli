{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sea Surface Current Prediction using LSTM\n",
    "\n",
    "This notebook outlines the process of loading, inspecting, and preprocessing sea surface current data (`u` and `v` components) for training an LSTM model. The goal is to predict sea surface currents for the next 24 hours based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect Data\n",
    "\n",
    "The initial step involves loading the dataset using xarray, which facilitates working with multi-dimensional arrays, and inspecting its structure to understand the available dimensions, coordinates, and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 168, lat: 52, lon: 43)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2023-01-01 ... 2023-01-07T23:00:00\n",
      "  * lat      (lat) float32 35.74 35.77 35.79 35.81 ... 36.81 36.84 36.86 36.88\n",
      "  * lon      (lon) float32 13.68 13.72 13.76 13.8 ... 15.26 15.3 15.34 15.38\n",
      "Data variables:\n",
      "    u        (time, lat, lon) float64 ...\n",
      "    v        (time, lat, lon) float64 ...\n",
      "    stdu     (time, lat, lon) float64 ...\n",
      "    stdv     (time, lat, lon) float64 ...\n",
      "    cov      (time, lat, lon) float64 ...\n",
      "    velo     (time, lat, lon) float64 ...\n",
      "    head     (time, lat, lon) float64 ...\n",
      "Attributes: (12/17)\n",
      "    NC_GLOBAL.Title:                   Near-Real time Surface Ocean Velocity\n",
      "    NC_GLOBAL.origin:                  MRAG (measured);LICA (measured);SOPU (...\n",
      "    NC_GLOBAL.source:                  HF Radar Derived Surface Currents obta...\n",
      "    NC_GLOBAL.history:                 08-Jun-2023 19:17:47\n",
      "    NC_GLOBAL.grid_type:               REGULAR\n",
      "    NC_GLOBAL.Conventions:             CF-1.4\n",
      "    ...                                ...\n",
      "    NC_GLOBAL.grid_resolution:         3.0km\n",
      "    NC_GLOBAL.geospatial_lat_max:      36.8802\n",
      "    NC_GLOBAL.geospatial_lat_min:      35.7447\n",
      "    NC_GLOBAL.geospatial_lon_max:      15.3804\n",
      "    NC_GLOBAL.geospatial_lon_min:      13.6768\n",
      "    NC_GLOBAL.netcdf_library_version:  v2 \n",
      "\n",
      "===============================================================================================================================================================================\n",
      "\n",
      "Extracted latitude values:\n",
      "[35.7447 35.767  35.7892 35.8115 35.8338 35.856  35.8783 35.9006 35.9228\n",
      " 35.9451 35.9673 35.9896 36.0119 36.0341 36.0564 36.0787 36.1009 36.1232\n",
      " 36.1455 36.1677 36.19   36.2123 36.2345 36.2568 36.2791 36.3013 36.3236\n",
      " 36.3458 36.3681 36.3904 36.4126 36.4349 36.4572 36.4794 36.5017 36.524\n",
      " 36.5462 36.5685 36.5908 36.613  36.6353 36.6576 36.6798 36.7021 36.7243\n",
      " 36.7466 36.7689 36.7911 36.8134 36.8357 36.8579 36.8802] \n",
      "\n",
      "\n",
      "Extracted longitude values:\n",
      "[13.6768 13.7174 13.7579 13.7985 13.839  13.8796 13.9202 13.9607 14.0013\n",
      " 14.0419 14.0824 14.123  14.1635 14.2041 14.2447 14.2852 14.3258 14.3664\n",
      " 14.4069 14.4475 14.488  14.5286 14.5692 14.6097 14.6503 14.6908 14.7314\n",
      " 14.772  14.8125 14.8531 14.8937 14.9342 14.9748 15.0153 15.0559 15.0965\n",
      " 15.137  15.1776 15.2182 15.2587 15.2993 15.3398 15.3804] \n",
      "\n",
      "===============================================================================================================================================================================\n",
      "\n",
      "Dataset start time: 2023-01-01T00:00:00.000000000 \n",
      "\n",
      "Dataset end time: 2023-01-07T23:00:00.000000000 \n",
      "\n",
      "===============================================================================================================================================================================\n",
      "\n",
      "Time range:\n",
      "DatetimeIndex(['2023-01-01 00:00:00', '2023-01-01 01:00:00',\n",
      "               '2023-01-01 02:00:00', '2023-01-01 03:00:00',\n",
      "               '2023-01-01 04:00:00', '2023-01-01 05:00:00',\n",
      "               '2023-01-01 06:00:00', '2023-01-01 07:00:00',\n",
      "               '2023-01-01 08:00:00', '2023-01-01 09:00:00',\n",
      "               ...\n",
      "               '2023-01-07 14:00:00', '2023-01-07 15:00:00',\n",
      "               '2023-01-07 16:00:00', '2023-01-07 17:00:00',\n",
      "               '2023-01-07 18:00:00', '2023-01-07 19:00:00',\n",
      "               '2023-01-07 20:00:00', '2023-01-07 21:00:00',\n",
      "               '2023-01-07 22:00:00', '2023-01-07 23:00:00'],\n",
      "              dtype='datetime64[ns]', length=168, freq='H') \n",
      "\n",
      "===============================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "file_path = 'Data/Processed_SSC_Data.nc'\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Print dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(ds, \"\\n\")\n",
    "print(\"=\"*175)\n",
    "\n",
    "# Extract latitude and longitude values directly from the dataset\n",
    "lats = ds['lat'].values\n",
    "lons = ds['lon'].values\n",
    "\n",
    "# Print the extracted latitude and longitude values\n",
    "print(\"\\nExtracted latitude values:\")\n",
    "print(lats, \"\\n\")\n",
    "print(\"\\nExtracted longitude values:\")\n",
    "print(lons, \"\\n\")\n",
    "print(\"=\"*175)\n",
    "\n",
    "start_time = ds['time'].values[0]\n",
    "end_time = ds['time'].values[-1]\n",
    "\n",
    "# Print the start and end times from the dataset\n",
    "print(\"\\nDataset start time:\", start_time, \"\\n\")\n",
    "print(\"Dataset end time:\", end_time, \"\\n\")\n",
    "print(\"=\"*175)\n",
    "\n",
    "# Create a time range for the predictions\n",
    "times = pd.date_range(start=start_time, end=end_time, freq='H')\n",
    "\n",
    "# Print the created time range for predictions\n",
    "print(\"\\nTime range:\")\n",
    "print(times, \"\\n\")\n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Prepare the data for LSTM modeling, which involves several key steps:\n",
    "\n",
    "1. Conversion to DataFrame: Convert the dataset to a pandas DataFrame for easier manipulation.\n",
    "2. Handling NaN Values: Fill NaN values with the mean of each column.\n",
    "3. Normalization: Scale the 'u' and 'v' features to a range between 0 and 1.\n",
    "4. Sequence Creation: Generate input sequences for the LSTM model.\n",
    "5. Train-Test Split: Divide the data into training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created with shape: (375648, 10)\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = ds.to_dataframe().reset_index()\n",
    "print(f\"DataFrame created with shape: {df.shape}\")\n",
    "\n",
    "# Handle NaN values by filling them with the mean of each column\n",
    "df['u'] = df['u'].fillna(df['u'].mean())\n",
    "df['v'] = df['v'].fillna(df['v'].mean())\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Assuming 'u' and 'v' are the features you want to normalize\n",
    "features = df[['u', 'v']]\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Replace 'u' and 'v' in df with their scaled versions\n",
    "df[['u', 'v']] = features_scaled\n",
    "\n",
    "# Define input_data and target_data\n",
    "input_data = df[['u', 'v']]  # Features used for prediction\n",
    "target_data = df[['u', 'v']]  # Targets to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences created. Input shape: (375601, 24, 2), Target shape: (375601, 24, 2)\n"
     ]
    }
   ],
   "source": [
    "# Updated function to create sequences with a corresponding target sequence for the next 24 hours\n",
    "def create_sequences(input_data, target_data, sequence_length, prediction_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(input_data) - sequence_length - prediction_length + 1):\n",
    "        X.append(input_data.iloc[i:i+sequence_length].values)\n",
    "        y.append(target_data.iloc[i+sequence_length:i+sequence_length+prediction_length].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters for sequence creation\n",
    "sequence_length = 24  # Use 24 hours of data to predict the next 24 hours\n",
    "prediction_length = 24\n",
    "\n",
    "X, y = create_sequences(input_data, target_data, sequence_length, prediction_length)\n",
    "print(f\"Sequences created. Input shape: {X.shape}, Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training, validation, and testing sets. Training shape: (262920, 24, 2), Validation shape: (56340, 24, 2), Testing shape: (56341, 24, 2)\n",
      "\n",
      "Training data contains NaN: False\n",
      "Validation data contains NaN: False\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training (70%), and a temporary test set (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "# Splitting the temporary test set equally into validation and test sets (50% of 30% -> 15% of the total data each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "print(f\"Data split into training, validation, and testing sets. Training shape: {X_train.shape}, Validation shape: {X_val.shape}, Testing shape: {X_test.shape}\\n\")\n",
    "\n",
    "# Check for NaNs or infinite values in the data\n",
    "print(f\"Training data contains NaN: {np.isnan(X_train).any() or np.isnan(y_train).any()}\")\n",
    "print(f\"Validation data contains NaN: {np.isnan(X_val).any() or np.isnan(y_val).any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LSTM Model Architecture\n",
    "\n",
    "Define an LSTM model architecture for our time series forecasting problem. The model will consist of an LSTM layer followed by a Dense layer to predict the next 24 hours of `u` and `v` values based on the previous 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 24, 128)          34304     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 24, 128)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 48)                2448      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 142,018\n",
      "Trainable params: 142,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a simpler LSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, activation='tanh', return_sequences=True, input_shape=(24, 2))),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(64, activation='tanh')),\n",
    "    Dropout(0.3),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(24 * 2)  # Predicting 24 hours ahead for both 'u' and 'v'\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model.build(input_shape=(None, 24, 2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.001, clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Compiling the model with an appropriate optimizer and loss function. Since this is a regression problem (predicting continuous `u` and `v` values), we will use the Mean Squared Error (MSE) loss function. The Adam optimizer will be used for its efficiency in handling sparse gradients and adaptive learning rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "Training the model on the training set while validating its performance on the validation set. We will use a batch size of 64 and train for 50 epochs. Early stopping will be employed to halt training when the validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 223/2055 [==>...........................] - ETA: 2:32 - loss: 0.0453"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    min_delta=0.00001,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "# Define model checkpoint\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'Saved_Models/best_model.h5', \n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model with both callbacks\n",
    "history = model.fit(\n",
    "    X_train, y_train.reshape(y_train.shape[0], -1),\n",
    "    epochs=100, batch_size=128,\n",
    "    validation_data=(X_val, y_val.reshape(y_val.shape[0], -1)),\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Plotting code\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model on Test Set\n",
    "\n",
    "Finally, we will evaluate the model's performance on the test set to assess its generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test, y_test.reshape(y_test.shape[0], -1), verbose=0)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reshape y_test and y_pred as needed to calculate metrics\n",
    "y_true = y_test.reshape(y_test.shape[0], -1)\n",
    "y_pred = y_pred.reshape(y_pred.shape[0], -1)\n",
    "\n",
    "# Calculate the different metrics\n",
    "test_mse = mean_squared_error(y_true, y_pred)   # Lower is better. Range: [0, ∞), 0 being perfect.\n",
    "test_mae = mean_absolute_error(y_true, y_pred)  # Lower is better. Range: [0, ∞), 0 being perfect.\n",
    "test_rmse = np.sqrt(test_mse)                   # Lower is better. Range: [0, ∞), 0 being perfect.\n",
    "\n",
    "# Print the metrics with explanations\n",
    "print(f'\\nTest Loss (MSE): {test_loss}')\n",
    "print(f'Test MAE: {test_mae}')\n",
    "print(f'Test MSE: {test_mse}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Real Life Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Input NetCDF File and Convert to DataFrame\n",
    "\n",
    "This file was extracted from the original dataset and will be used as input to the model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input NetCDF file (this file will be inputted into the model to make predictions)\n",
    "input_ds = xr.open_dataset('Data/sea_surface_currents_input_for_prediction.nc')\n",
    "\n",
    "# Convert file to DataFrame\n",
    "input_df = input_ds.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing input file \n",
    "\n",
    "Similar steps as the training set: handle NaNs, normalize, create sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values by filling them with the mean of each column\n",
    "input_df['u'] = input_df['u'].fillna(input_df['u'].mean())\n",
    "input_df['v'] = input_df['v'].fillna(input_df['v'].mean())\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Normalize 'u' and 'v' features\n",
    "features = input_df[['u', 'v']]\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Replace 'u' and 'v' in input_df with their scaled versions\n",
    "input_df[['u', 'v']] = features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reshape Input Data for LSTM Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last 24-hour sequence of data for prediction\n",
    "# Note: Adjust the slicing based on how your data is structured and how the model was trained\n",
    "last_24_hours_df = input_df[-24:]\n",
    "\n",
    "### ADD DEBUG\n",
    "\n",
    "# Reshape data for the LSTM model input\n",
    "# LSTM model was trained on 24-hour sequences with 2 features ('u' and 'v')\n",
    "X_predict = last_24_hours_df[['u', 'v']].to_numpy().reshape(1, 24, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions and Reshape Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the LSTM model\n",
    "predicted_data = model.predict(X_predict, verbose=0)\n",
    "\n",
    "# Inverse transform the prediction to the original scale\n",
    "predicted_data_rescaled = scaler.inverse_transform(predicted_data.reshape(24, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output (predicted data) to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the prediction dates, latitudes, and longitudes available\n",
    "prediction_dates = pd.date_range(start=\"2023-06-02\", periods=24, freq='h')\n",
    "lats = input_ds['lat'].values  # Assuming input_ds is your input xarray Dataset\n",
    "lons = input_ds['lon'].values\n",
    "\n",
    "# Initialize arrays to hold the duplicated predictions\n",
    "predicted_u_expanded = np.tile(predicted_data_rescaled[:, 0], (len(lats), len(lons), 1)).transpose((2, 0, 1))\n",
    "predicted_v_expanded = np.tile(predicted_data_rescaled[:, 1], (len(lats), len(lons), 1)).transpose((2, 0, 1))\n",
    "\n",
    "# Now, create the predicted_ds Dataset with the expanded predictions\n",
    "predicted_ds = xr.Dataset(\n",
    "    {\n",
    "        \"u\": ((\"time\", \"lat\", \"lon\"), predicted_u_expanded),\n",
    "        \"v\": ((\"time\", \"lat\", \"lon\"), predicted_v_expanded),\n",
    "    },\n",
    "    coords={\n",
    "        \"time\": prediction_dates,\n",
    "        \"lat\": lats,\n",
    "        \"lon\": lons,\n",
    "    }\n",
    ")\n",
    "\n",
    "file_path = 'Data/predicted_sea_surface_currents.nc'\n",
    "\n",
    "# Save the predicted dataset to a NetCDF file\n",
    "predicted_ds.to_netcdf(file_path)\n",
    "\n",
    "# Load your dataset\n",
    "predicted_ds_check = xr.open_dataset(file_path)\n",
    "\n",
    "# Print dataset structure\n",
    "print(\"Predicted Dataset structure:\")\n",
    "print(predicted_ds_check, \"\\n\")\n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Prediction to Actual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_2 = 'Data/sea_surface_currents_compare_for_prediction.nc'\n",
    "\n",
    "# Load the comparison file\n",
    "compare_ds = xr.open_dataset(file_path_2)\n",
    "\n",
    "# Output information to see if it is correct date and format\n",
    "compare_ds_check = xr.open_dataset(file_path_2)\n",
    "\n",
    "# Print dataset structure\n",
    "print(\"Comarison File Dataset structure:\")\n",
    "print(compare_ds_check, \"\\n\")\n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the actual 'u' and 'v' values for comparison\n",
    "actual_u = compare_ds['u'].values.flatten()\n",
    "actual_v = compare_ds['v'].values.flatten()\n",
    "\n",
    "# Flatten the predicted 'u' and 'v' values for comparison\n",
    "predicted_u = predicted_ds['u'].values.flatten()\n",
    "predicted_v = predicted_ds['v'].values.flatten()\n",
    "\n",
    "# Filter out NaNs from actual_u and actual_v and their corresponding predictions\n",
    "not_nan_mask_u = ~np.isnan(actual_u)\n",
    "filtered_actual_u = actual_u[not_nan_mask_u]\n",
    "filtered_predicted_u = predicted_u[not_nan_mask_u]\n",
    "\n",
    "not_nan_mask_v = ~np.isnan(actual_v)\n",
    "filtered_actual_v = actual_v[not_nan_mask_v]\n",
    "filtered_predicted_v = predicted_v[not_nan_mask_v]\n",
    "\n",
    "# Calculate error metrics for 'u' using filtered values\n",
    "mse_u = mean_squared_error(filtered_actual_u, filtered_predicted_u)\n",
    "mae_u = mean_absolute_error(filtered_actual_u, filtered_predicted_u)\n",
    "rmse_u = np.sqrt(mse_u)\n",
    "\n",
    "# Calculate error metrics for 'v' using filtered values\n",
    "mse_v = mean_squared_error(filtered_actual_v, filtered_predicted_v)\n",
    "mae_v = mean_absolute_error(filtered_actual_v, filtered_predicted_v)\n",
    "rmse_v = np.sqrt(mse_v)\n",
    "\n",
    "print(f\"Error Metrics for 'u':\\nMSE: {mse_u}\\nMAE: {mae_u}\\nRMSE: {rmse_u}\\n\")\n",
    "print(f\"Error Metrics for 'v':\\nMSE: {mse_v}\\nMAE: {mae_v}\\nRMSE: {rmse_v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making comparisons for same location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_lon = 13.68\n",
    "selected_lat = 35.74\n",
    "\n",
    "print(f\"Filtered Actual vs Predicted 'u' values for the first 24 hours at location (lon: {selected_lon}, lat: {selected_lat}):\")\n",
    "for i in range(24):\n",
    "    # We need to check if we have enough data points after filtering\n",
    "    if i < len(filtered_actual_u) and i < len(filtered_predicted_u):\n",
    "        print(f\"Hour {i+1}: Actual u: {filtered_actual_u[i]}, Predicted u: {filtered_predicted_u[i]}\")\n",
    "    else:\n",
    "        print(f\"Hour {i+1}: Not enough data points available after filtering for NaN values.\")\n",
    "\n",
    "print(f\"\\nFiltered Actual vs Predicted 'v' values for the first 24 hours at location (lon: {selected_lon}, lat: {selected_lat}):\")\n",
    "for i in range(24):\n",
    "    if i < len(filtered_actual_v) and i < len(filtered_predicted_v):\n",
    "        print(f\"Hour {i+1}: Actual v: {filtered_actual_v[i]}, Predicted v: {filtered_predicted_v[i]}\")\n",
    "    else:\n",
    "        print(f\"Hour {i+1}: Not enough data points available after filtering for NaN values.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
