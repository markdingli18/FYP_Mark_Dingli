{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sea Surface Current Prediction using LSTM\n",
    "\n",
    "This notebook outlines the process of loading, inspecting, and preprocessing sea surface current data (`u` and `v` components) for training an LSTM model. The goal is to predict sea surface currents for the next 24 hours based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.path as mpath\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout, InputLayer\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect Data\n",
    "\n",
    "The initial step involves loading the dataset using xarray, which facilitates working with multi-dimensional arrays, and inspecting its structure to understand the available dimensions, coordinates, and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "netcdf_file_path = \"Data/24_months_SSC_Data.nc\"\n",
    "\n",
    "# Open the data\n",
    "dataset = xr.open_dataset(netcdf_file_path)\n",
    "\n",
    "# Print head of dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a pandas DataFrame\n",
    "df = dataset.to_dataframe().reset_index()\n",
    "\n",
    "# Drop columns from the DataFrame\n",
    "df = df.drop(columns=['stdu', 'stdv', 'cov', 'velo', 'head'])\n",
    "\n",
    "# Remove rows where 'u' and 'v' have NaN values\n",
    "df = df.dropna(subset=['u', 'v'])\n",
    "\n",
    "# Ensure no NaN values remain in the DataFrame\n",
    "assert not df.isnull().values.any(), \"There are still NaN values in the DataFrame\"\n",
    "\n",
    "# Convert the 'time' column to a datetime object\n",
    "df.index = pd.to_datetime(df['time'], format='%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Visualization\n",
    "Define boundaries and coordinates for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the boundaries for map\n",
    "boundaries = {\n",
    "    'min_lon': 14.15,  \n",
    "    'max_lon': 14.81,  \n",
    "    'min_lat': 35.79,  \n",
    "    'max_lat': 36.3   \n",
    "}\n",
    "\n",
    "# Define the coordinates \n",
    "polygon_coordinates = [\n",
    "    (14.6, 35.87), \n",
    "    (14.35, 36.01), \n",
    "    (14.35, 36.09), \n",
    "    (14.6, 36.09), \n",
    "    (14.6, 35.87)\n",
    "]\n",
    "\n",
    "# Create a plot with Cartopy\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.set_extent([boundaries['min_lon'], boundaries['max_lon'], boundaries['min_lat'], boundaries['max_lat']], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Plotting all points within the boundary from the dataframe 'df'\n",
    "ax.scatter(df['lon'], df['lat'], s=10, color='blue', marker='o', alpha=0.5, transform=ccrs.Geodetic())\n",
    "\n",
    "# Create a red polygon and add it to the plot\n",
    "red_polygon = mpatches.Polygon(polygon_coordinates, closed=True, edgecolor='red', facecolor='none', linewidth=3, transform=ccrs.Geodetic())\n",
    "ax.add_patch(red_polygon)\n",
    "\n",
    "# Gridlines and labels\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering \n",
    "Filter the DataFrame to include only data points within the specified polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the polygon coordinates to a Path object\n",
    "polygon_path = mpath.Path(polygon_coordinates)\n",
    "\n",
    "# Use the Path object to find points inside the polygon\n",
    "inside_polygon = df.apply(lambda row: polygon_path.contains_point((row['lon'], row['lat'])), axis=1)\n",
    "df_inside_polygon = df[inside_polygon]\n",
    "\n",
    "# Create a plot with Cartopy\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "ax.set_extent([boundaries['min_lon'], boundaries['max_lon'], boundaries['min_lat'], boundaries['max_lat']], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Plotting only the points within the polygon\n",
    "ax.scatter(df_inside_polygon['lon'], df_inside_polygon['lat'], s=10, color='red', marker='x', alpha=0.5, transform=ccrs.Geodetic())\n",
    "\n",
    "# Gridlines and labels\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the polygon coordinates to a Path object\n",
    "polygon_path = mpath.Path(polygon_coordinates)\n",
    "\n",
    "# Prepare the coordinates as a list of tuples\n",
    "coordinates = list(zip(df['lon'], df['lat']))\n",
    "\n",
    "# Check if each coordinate is inside the polygon\n",
    "inside_mask = [polygon_path.contains_point(coord) for coord in coordinates]\n",
    "\n",
    "# Filter the DataFrame for points within the polygon\n",
    "df_inside_polygon = df[inside_mask]\n",
    "\n",
    "# Removing potential duplicates\n",
    "df_inside_polygon = df_inside_polygon.drop_duplicates(subset=['lon', 'lat'])\n",
    "\n",
    "# Count the number of points inside the polygon\n",
    "num_points_inside_polygon = df_inside_polygon.shape[0]\n",
    "print(f\"Total number of points inside the polygon: {num_points_inside_polygon}\\n\")\n",
    "print(\"Coordinates of the points inside the polygon:\")\n",
    "\n",
    "# Print out all of the coordinate pairs\n",
    "coordinates_inside = df_inside_polygon[['lon', 'lat']].values\n",
    "for lon, lat in coordinates_inside:\n",
    "    print(f\"({lon}, {lat})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'inside_mask' is a list of booleans indicating whether each point is inside the polygon\n",
    "inside_mask = [polygon_path.contains_point((lon, lat)) for lon, lat in zip(df['lon'], df['lat'])]\n",
    "\n",
    "# Convert 'inside_mask' to a pandas Series to use as a boolean indexer\n",
    "inside_series = pd.Series(inside_mask, index=df.index)\n",
    "\n",
    "# Filter the original DataFrame using the boolean Series\n",
    "final_df = df[inside_series]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exctract data only for a single pair of coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates \n",
    "target_lat = 36.03409957885742\n",
    "target_lon = 14.528599739074707\n",
    "\n",
    "# Filter the DataFrame for the exact coordinates\n",
    "df_point = final_df[(final_df['lat'] == target_lat) & (final_df['lon'] == target_lon)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_u = df_point['u']\n",
    "\n",
    "plt.figure(figsize=(10, 5))  \n",
    "plt.plot(df1_u.index, df1_u, label='u component')  \n",
    "plt.title('Sea Surface Current \"u\" Component Over Time')  \n",
    "plt.xlabel('Time')  \n",
    "plt.ylabel('u Component Value')  \n",
    "plt.legend()  \n",
    "plt.grid(True)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the time series data into a format suitable for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_y(df_u, df_v, window_size=5):\n",
    "    df_u_as_np = df_u.to_numpy()\n",
    "    df_v_as_np = df_v.to_numpy()\n",
    "    X = [] \n",
    "    y = []\n",
    "    for i in range(len(df_u_as_np) - window_size):\n",
    "        # Create a combined feature for each time step in the window\n",
    "        combined_features = [[df_u_as_np[i+j], df_v_as_np[i+j]] for j in range(window_size)]\n",
    "        X.append(combined_features)\n",
    "        # The target remains the 'u' value at the next time step\n",
    "        label = df_u_as_np[i + window_size]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 1\n",
    "X, y = df_to_X_y(df_point['u'], df_point['v'], window_size=window_size)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, train_size, val_size, test_size):\n",
    "    # Calculate the indices for the end of each segment\n",
    "    total_size = len(X)\n",
    "    train_end = int(total_size * train_size)\n",
    "    val_end = train_end + int(total_size * val_size)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train = X[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    X_val = X[train_end:val_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    \n",
    "    X_test = X[val_end:]\n",
    "    y_test = y[val_end:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "train_size = 0.70\n",
    "val_size = 0.15\n",
    "test_size = 1 - (train_size + val_size)\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X, y, train_size, val_size, test_size)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define & Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GRU model\n",
    "model = Sequential()\n",
    "model.add(InputLayer((window_size, 2)))\n",
    "model.add(GRU(64, return_sequences=True))  # Use one GRU layer with fewer units\n",
    "model.add(Dropout(0.2))  # Regularization with dropout\n",
    "model.add(GRU(32))  # Last GRU does not return sequences\n",
    "model.add(Dropout(0.2))  # Regularization with dropout\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=l2(0.01)))  # Dense layer with L2 regularization\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define ModelCheckpoint callback to save the best model\n",
    "model_checkpoint = ModelCheckpoint(filepath=\"Data/GRU/Saved_Models/gru_single_u_target_model\",\n",
    "                     save_best_only=True,\n",
    "                     monitor='val_loss',  \n",
    "                     mode='min',\n",
    "                     verbose=0)   \n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=8,\n",
    "                               restore_best_weights=True,\n",
    "                               min_delta=0.0001,  \n",
    "                               verbose=1)             \n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=MeanSquaredError(),\n",
    "              optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with the training data and validate with the validation data\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,\n",
    "                    callbacks=[model_checkpoint, early_stopping])\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model(\"Data/GRU/Saved_Models/gru_single_u_target_model\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"test_predictions shape:\", test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "mse_u = mean_squared_error(y_test, test_predictions)\n",
    "m_ae_u = mean_absolute_error(y_test, test_predictions)\n",
    "r2_u = r2_score(y_test, test_predictions)\n",
    "explained_variance_u = explained_variance_score(y_test, test_predictions)\n",
    "\n",
    "# Output the error metrics\n",
    "print(\"Test MSE for 'u' component:\", mse_u)\n",
    "print(\"Test MAE for 'u' component:\", m_ae_u)\n",
    "print(\"Test R-squared for 'u' component:\", r2_u)\n",
    "print(\"Test Explained Variance for 'u' component:\", explained_variance_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame({\n",
    "    'Test Predictions u': test_predictions.flatten(),\n",
    "    'Actuals u': y_test\n",
    "})\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for the 'u' component test predictions vs actuals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(test_results.index, test_results['Test Predictions u'], label='Test Predictions u', color='blue', linestyle='--')\n",
    "plt.plot(test_results.index, test_results['Actuals u'], label='Actuals u', color='red', alpha=0.7)\n",
    "plt.title('Test Predictions vs Actuals for u Component')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making 24-Hour Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first 24 predictions for comparison\n",
    "last_24_predictions = test_predictions[-24:]\n",
    "last_24_actual = y_test[-24:]\n",
    "\n",
    "# Flatten the predictions to make them a 1D array, if they're not already\n",
    "first_24_predictions = last_24_predictions.flatten()\n",
    "\n",
    "# Calculate metrics for the first 24-hour predictions\n",
    "mse_24h = mean_squared_error(last_24_actual, first_24_predictions)\n",
    "mae_24h = mean_absolute_error(last_24_actual, first_24_predictions)\n",
    "r2_24h = r2_score(last_24_actual, first_24_predictions)\n",
    "explained_variance_24h = explained_variance_score(last_24_actual, first_24_predictions)\n",
    "\n",
    "# Output the error metrics for the 24-hour predictions\n",
    "print(\"First 24-Hour MSE:\", mse_24h)\n",
    "print(\"First 24-Hour MAE:\", mae_24h)\n",
    "print(\"First 24-Hour R-squared:\", r2_24h)\n",
    "print(\"First 24-Hour Explained Variance:\", explained_variance_24h)\n",
    "\n",
    "# Plot for the first 24-hour predictions vs actuals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(24), last_24_actual, label='Actual', marker='o', color='red')\n",
    "plt.plot(range(24), first_24_predictions, label='Predicted', marker='x', color='blue', linestyle='--')\n",
    "plt.title('First 24-Hour Forecast vs Actual')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('u Component Value')\n",
    "plt.xticks(range(24))  # Set x-ticks to show each hour\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
