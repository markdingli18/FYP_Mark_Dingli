{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983ab358",
   "metadata": {},
   "source": [
    "# NetCDF File Processing for Sea Surface Currents (SSC) Data\n",
    "This notebook contains the process for automatically merging NetCDF files corresponding to specified start and end dates. The merged file will contain concatenated data along the time dimension for the sea surface currents data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d53279",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a773b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc5b38",
   "metadata": {},
   "source": [
    "## Specify Start and End Dates\n",
    "Set the `start_date` and `end_date` variables to the desired range of dates for which you want to merge NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bf5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end date of the simulation or observation period\n",
    "# Format: 'YYYY-MM-DD' (Year-Month-Day)\n",
    "start_date = '2023-08-08' \n",
    "end_date = '2023-08-08'   \n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = \"Data/1_day_SSC_Data.nc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3cbea",
   "metadata": {},
   "source": [
    "## Generate File Paths\n",
    "The function `generate_file_paths` creates a list of file paths for the NetCDF files that fall within the specified date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a1027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_paths(start_date, end_date, base_directory):\n",
    "    # Convert start and end dates from string to datetime objects\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Extend the end date by one day to ensure the range includes all hours of the last day\n",
    "    end_dt += timedelta(days=1)\n",
    "\n",
    "    # Create a Pandas date range of hourly intervals from start to the end of the last day\n",
    "    # The last hour is excluded as it technically belongs to the following day\n",
    "    dates = pd.date_range(start_dt, end_dt, freq='h')[:-1]  # Hourly frequency, excluding the first hour of the next day\n",
    "    \n",
    "    # Initialize a list to store the generated file paths\n",
    "    file_paths = []\n",
    "    # Loop through each date in the range to construct file paths\n",
    "    for date in dates:\n",
    "        # Format the file name based on the date and time\n",
    "        file_name = f\"CODAR_MALT_{date.strftime('%Y_%m_%d_%H%M')}-{int(date.timestamp())}.nc\"\n",
    "        # Construct the full file path using the base directory and date components\n",
    "        file_path = Path(base_directory) / f\"SSC_MaltaSicily_{date.year}\" / str(date.year) / date.strftime('%m') / date.strftime('%d') / file_name\n",
    "        # Check if the file path exists before adding it to the list\n",
    "        if file_path.exists():\n",
    "            file_paths.append(str(file_path))\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "# Base path to NetCDF files\n",
    "base_directory = \"Data/sea_surface_currents\"\n",
    "\n",
    "# Generate the file paths for the specified date range\n",
    "file_paths = generate_file_paths(start_date, end_date, base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad779c0",
   "metadata": {},
   "source": [
    "## Merge NetCDF Files\n",
    "The following code merges all NetCDF files from the generated file paths into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50141c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All NetCDF files have been merged into Data/1_day_SSC_Data.nc\n",
      "===============================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def merge_netcdf_files(file_paths, output_file_path):\n",
    "    # Load all NetCDF files into a list of xarray datasets\n",
    "    datasets = [xr.open_dataset(fp) for fp in file_paths]\n",
    "    \n",
    "    # Merge all datasets into one along the time dimension\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    \n",
    "    # Save the combined dataset to a new NetCDF file\n",
    "    combined_dataset.to_netcdf(output_file_path)\n",
    "    \n",
    "    # Close all datasets to free up resources\n",
    "    for ds in datasets:\n",
    "        ds.close()\n",
    "    \n",
    "    print(f\"All NetCDF files have been merged into {output_file_path}\")\n",
    "    print(\"=\"*175)\n",
    "\n",
    "# Merge the NetCDF files\n",
    "merge_netcdf_files(file_paths, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ac8f1",
   "metadata": {},
   "source": [
    "### Verify Merged Dataset\n",
    "\n",
    "This code opens the merged NetCDF file and verifies its contents, ensuring that the dimensions, coordinates, and variables are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9f2765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Sea Surface Currents Dataset Information\n",
      "=============================================================================================================================\n",
      "\n",
      "Dataset Dimensions:\n",
      "FrozenMappingWarningOnValuesAccess({'time': 24, 'lat': 52, 'lon': 43})\n",
      "\n",
      "Dataset Coordinates:\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2023-08-08 ... 2023-08-08T23:00:00\n",
      "  * lat      (lat) float32 35.74 35.77 35.79 35.81 ... 36.81 36.84 36.86 36.88\n",
      "  * lon      (lon) float32 13.68 13.72 13.76 13.8 ... 15.26 15.3 15.34 15.38\n",
      "\n",
      "Data Variables in the Dataset:\n",
      "Data variables:\n",
      "    u        (time, lat, lon) float64 ...\n",
      "    v        (time, lat, lon) float64 ...\n",
      "    stdu     (time, lat, lon) float64 ...\n",
      "    stdv     (time, lat, lon) float64 ...\n",
      "    cov      (time, lat, lon) float64 ...\n",
      "    velo     (time, lat, lon) float64 ...\n",
      "    head     (time, lat, lon) float64 ...\n",
      "\n",
      "Attributes (Metadata) in the Dataset:\n",
      "{'NC_GLOBAL.Title': 'Near-Real time Surface Ocean Velocity', 'NC_GLOBAL.origin': 'BARK (measured);POZZ (measured);MRAG (measured);LICA (measured);SOPU (measured);', 'NC_GLOBAL.source': 'HF Radar Derived Surface Currents obtained from CODAR combine method', 'NC_GLOBAL.history': '08-Aug-2023 00:50:09', 'NC_GLOBAL.grid_type': 'REGULAR', 'NC_GLOBAL.Conventions': 'CF-1.4', 'NC_GLOBAL.creator_url': 'http://www.qualitasinstruments.com/', 'NC_GLOBAL.institution': 'Qualitas Remos', 'NC_GLOBAL.creator_name': 'Macu Ferrer', 'NC_GLOBAL.creator_email': 'macu.ferrer@qualitasremos.com', 'NC_GLOBAL.grid_projection': 'equidistal cylindrical', 'NC_GLOBAL.grid_resolution': '3.0km', 'NC_GLOBAL.geospatial_lat_max': 36.8802, 'NC_GLOBAL.geospatial_lat_min': 35.7447, 'NC_GLOBAL.geospatial_lon_max': 15.3804, 'NC_GLOBAL.geospatial_lon_min': 13.6768, 'NC_GLOBAL.netcdf_library_version': 'v2'}\n",
      "Time Points: 24\n",
      "\n",
      "Latitude Range in the Dataset: 35.7447 to 36.8802\n",
      "Longitude Range in the Dataset: 13.6768 to 15.3804\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Open the merged dataset\n",
    "ds = xr.open_dataset(output_file_path)\n",
    "\n",
    "# Print dataset information\n",
    "print(\"=\" * 125)\n",
    "print(\"Sea Surface Currents Dataset Information\")\n",
    "print(\"=\" * 125)\n",
    "print(\"\\nDataset Dimensions:\")\n",
    "print(ds.dims)\n",
    "print(\"\\nDataset Coordinates:\")\n",
    "print(ds.coords)\n",
    "print(\"\\nData Variables in the Dataset:\")\n",
    "print(ds.data_vars)\n",
    "print(\"\\nAttributes (Metadata) in the Dataset:\")\n",
    "print(ds.attrs)\n",
    "\n",
    "# Verify the time dimension is as expected\n",
    "time_points = ds.sizes['time']\n",
    "print(\"Time Points:\", time_points)\n",
    "\n",
    "# Ensure the lat/lon are within the specified bounds\n",
    "lat_min, lat_max = ds['lat'].min().values, ds['lat'].max().values\n",
    "lon_min, lon_max = ds['lon'].min().values, ds['lon'].max().values\n",
    "print(\"\\nLatitude Range in the Dataset:\", lat_min, \"to\", lat_max)\n",
    "print(\"Longitude Range in the Dataset:\", lon_min, \"to\", lon_max)\n",
    "\n",
    "# Assert the presence of expected variables\n",
    "assert 'u' in ds.variables, \"u variable is missing from the dataset\"\n",
    "assert 'v' in ds.variables, \"v variable is missing from the dataset\"\n",
    "\n",
    "print(\"=\" * 125)\n",
    "\n",
    "# Close the dataset after inspection\n",
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
