{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Analysis of Sea Surface Currents Velocity 'u' (East-West)\n",
    "\n",
    "This Notebook is an analysis and prediction of sea surface currents (SSC) velocity using an LSTM Model. The primary focus is on processing, visualizing, and predicting based on historical sea surface current data. This pipeline aims to provide a comprehensive workflow from data processing to real-life future predictions, facilitating deeper insights into sea surface currents and their potential impacts.\n",
    "\n",
    "## Overview\n",
    "- **Data Preparation**: Loading and cleaning of time-series data from NetCDF files to create a structured dataset suitable for analysis.\n",
    "- **Geospatial Visualization**: Mapping sea surface current data and identify areas of interest.\n",
    "- **Data Filtering**: Narrowing down the dataset to focus on specific geographic regions and time frames for detailed analysis.\n",
    "- **Predictive Modeling**: Developing LSTM models to forecast future sea surface current velocities based on historical data.\n",
    "- **Evaluation**: Comparing predicted values against actual data to assess the accuracy and effectiveness of the models.\n",
    "- **Output**: Aggregating all predictions and converting them into a format suitable for further analysis or integration into a Lagrangian Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.path as mpath\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import MeanAbsoluteError\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppress Future Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing & Opening the Data\n",
    "\n",
    "Load and open the sea surface current dataset for subsequent analysis and modeling. This file has hourly data from 1st January 2020 till 1st August 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "netcdf_file_path = \"Data/model_IO/final_SSC_Data.nc\"\n",
    "\n",
    "# Open the dataset\n",
    "dataset = xr.open_dataset(netcdf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation \n",
    "\n",
    "1. **Conversion and Index Resetting**: The dataset is converted to a Pandas DataFrame and the index is reset.\n",
    "2. **Column Removal**: Unnecessary columns ('stdu', 'stdv', 'cov', 'velo', 'head') that are not needed are removed.\n",
    "3. **NaN Value Removal**: Rows containing NaN values in the 'u' and 'v' columns are removed.\n",
    "4. **Datetime Conversion**: The 'time' column is converted to a datetime format.\n",
    "5. **Display**: Finally, the cleaned DataFrame is displayed to verify the successful preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a pandas DataFrame and reset the index \n",
    "df = dataset.to_dataframe().reset_index()\n",
    "\n",
    "# Drop unnecessary columns from the DataFrame that are not needed\n",
    "df = df.drop(columns=['stdu', 'stdv', 'cov', 'velo', 'head'])\n",
    "\n",
    "# Drop rows with NaN values in 'u' and 'v' columns to ensure data integrity for analysis\n",
    "df.dropna(subset=['u', 'v'],inplace=True)\n",
    "\n",
    "# Convert the 'time' column to datetime format for time series analysis\n",
    "df.index = pd.to_datetime(df['time'], format='%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Visualization\n",
    "\n",
    "- **Boundaries & Interest Area**: Set the coordinates dor the specific geographic boundaries and a polygon to highlight key area on the map.\n",
    "- **Map Setup**: Initialize a map with features like land, coastlines, and borders for context.\n",
    "- **Data Points**: Plot data points within the set boundaries to visualize the distribution of sea surface currents.\n",
    "- **Highlight Area**: Draw a red polygon to emphasize the area of interest on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the boundaries for the map \n",
    "boundaries = {\n",
    "    'min_lon': 14.15,  \n",
    "    'max_lon': 14.81,  \n",
    "    'min_lat': 35.79,  \n",
    "    'max_lat': 36.3    \n",
    "}\n",
    "\n",
    "# Define the vertices of a polygon to highlight a specific area on the map\n",
    "polygon_coordinates = [\n",
    "    (14.6, 35.87),\n",
    "    (14.35, 36.01),\n",
    "    (14.35, 36.09),\n",
    "    (14.6, 36.09),\n",
    "    (14.6, 35.87)\n",
    "]\n",
    "\n",
    "# Initialize a matplotlib figure with Cartopy for data visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Set the viewing extent of the map using the defined boundaries\n",
    "ax.set_extent([boundaries['min_lon'], boundaries['max_lon'], boundaries['min_lat'], boundaries['max_lat']], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add features to the map for land, coastlines, and country borders for context\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Plot all data points from the dataframe within the defined geographic boundaries\n",
    "ax.scatter(df['lon'], df['lat'], s=10, color='blue', marker='o', alpha=0.5, transform=ccrs.Geodetic())\n",
    "\n",
    "# Draw a red polygon using the defined coordinates to highlight a specific area on the map\n",
    "red_polygon = mpatches.Polygon(polygon_coordinates, closed=True, edgecolor='red', facecolor='none', linewidth=3, transform=ccrs.Geodetic())\n",
    "ax.add_patch(red_polygon)\n",
    "\n",
    "# Add gridlines to the map\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Filtering\n",
    "\n",
    "Filter the dataset to focus on the area of interest:\n",
    "\n",
    "- **Filtering**: Apply the polygon as a filter to select only data points located inside this area.\n",
    "- **Visualization**: Plot the filtered data on a map to visually confirm the focus area, marking points within the polygon in red for clarity.\n",
    "- **Map Features**: Enhance the map with features like land, coastlines, and borders for better context.\n",
    "- **Final View**: Display the map with the filtered points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of polygon coordinates to a Path object\n",
    "polygon_path = mpath.Path(polygon_coordinates)\n",
    "\n",
    "# Use the Path object to filter points in the DataFrame that lie inside the polygon\n",
    "inside_polygon = df.apply(lambda row: polygon_path.contains_point((row['lon'], row['lat'])), axis=1)\n",
    "df_inside_polygon = df[inside_polygon]\n",
    "\n",
    "# Initialize a new matplotlib figure with Cartopy for mapping, specifying the projection\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Set the extent of the map to the defined geographic boundaries\n",
    "ax.set_extent([boundaries['min_lon'], boundaries['max_lon'], boundaries['min_lat'], boundaries['max_lat']], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add land, coastlines, and country borders to the map \n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "# Plot the points that are located within the defined polygon in red\n",
    "scatter = ax.scatter(df_inside_polygon['lon'], df_inside_polygon['lat'], s=10, color='red', marker='x', alpha=0.5, transform=ccrs.Geodetic(), label='Filtered Points')\n",
    "\n",
    "# Add gridlines and labels to the map\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# Add a legend to the plot to explain the red \"X\" markers\n",
    "ax.legend(handles=[scatter], loc='upper right')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the coordinates inside the polygon\n",
    "\n",
    "Identify and processes data points located within the selected area:\n",
    "\n",
    "- **Path Creation**: Transform the polygon coordinates into a Path object for spatial filtering.\n",
    "- **Coordinate Pairing**: Combine 'lon' and 'lat' from the DataFrame into coordinate tuples.\n",
    "- **Spatial Filtering**: Apply the Path object to select only points inside the polygon.\n",
    "- **Duplicate Removal**: Eliminate duplicate points to ensure unique data entries.\n",
    "- **Count and Display**: Calculate and print the total number of unique points inside the polygon, along with their coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the polygon coordinates to a Path object \n",
    "polygon_path = mpath.Path(polygon_coordinates)\n",
    "\n",
    "# Combine longitude and latitude columns from the DataFrame into a list of tuples for each point\n",
    "coordinates = list(zip(df['lon'], df['lat']))\n",
    "\n",
    "# Create a mask to identify which points fall within the defined polygon by checking each point\n",
    "inside_mask = [polygon_path.contains_point(coord) for coord in coordinates]\n",
    "\n",
    "# Use the mask to filter the DataFrame, keeping only points that are inside the polygon\n",
    "df_inside_polygon = df[inside_mask]\n",
    "\n",
    "# Remove potential duplicate entries based on their longitude and latitude\n",
    "df_inside_polygon = df_inside_polygon.drop_duplicates(subset=['lon', 'lat'])\n",
    "\n",
    "# Calculate the number of unique points found inside the polygon\n",
    "num_points_inside_polygon = df_inside_polygon.shape[0]\n",
    "print(f\"Total number of points inside the polygon: {num_points_inside_polygon}\\n\")\n",
    "print(\"Coordinates of the points inside the polygon:\")\n",
    "\n",
    "# Iterate over each coordinate pair inside the polygon and print them out\n",
    "coordinates_inside = df_inside_polygon[['lon', 'lat']].values\n",
    "for lon, lat in coordinates_inside:\n",
    "    print(f\"({lon}, {lat})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Data Frame for the coordinates inside the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'inside_mask' to a pandas Series to use as a boolean indexer\n",
    "inside_series = pd.Series(inside_mask, index=df.index)\n",
    "\n",
    "# Filter the original DataFrame using the boolean Series\n",
    "final_df = df[inside_series]\n",
    "\n",
    "# Display the final DataFrame\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction for Selected Coordinates\n",
    "\n",
    "Extracting and saving specific sea surface current data for predefined locations within our area of interest:\n",
    "\n",
    "- **Selected Coordinates**: List the specific latitude and longitude pairs.\n",
    "- **Data Extraction**: For each pair, filter the main dataset to obtain the relevant data points.\n",
    "- **File Saving**: Convert latitude and longitude to a format suitable for filenames and save the data as CSV files.\n",
    "\n",
    "Each data point's latitude and longitude are converted to remove decimal points, ensuring valid filenames. This process results in a set of CSV files, each corresponding to a unique location within the selected polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the files\n",
    "save_directory = \"Data/coordinate_data_frames\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Coordinates of the points inside the polygon\n",
    "coordinates_inside = [\n",
    "    (14.569199562072754, 35.90060043334961),\n",
    "    (14.569199562072754, 35.92279815673828),\n",
    "    (14.569199562072754, 35.945098876953125),\n",
    "    (14.528599739074707, 35.96730041503906),\n",
    "    (14.569199562072754, 35.96730041503906),\n",
    "    (14.447500228881836, 35.989601135253906),\n",
    "    (14.48799991607666, 35.989601135253906),\n",
    "    (14.528599739074707, 35.989601135253906),\n",
    "    (14.569199562072754, 35.989601135253906),\n",
    "    (14.447500228881836, 36.01190185546875),\n",
    "    (14.48799991607666, 36.01190185546875),\n",
    "    (14.528599739074707, 36.01190185546875),\n",
    "    (14.569199562072754, 36.01190185546875),\n",
    "    (14.447500228881836, 36.03409957885742),\n",
    "    (14.48799991607666, 36.03409957885742),\n",
    "    (14.528599739074707, 36.03409957885742),\n",
    "    (14.569199562072754, 36.03409957885742),\n",
    "    (14.447500228881836, 36.056400299072266),\n",
    "    (14.48799991607666, 36.056400299072266),\n",
    "    (14.528599739074707, 36.056400299072266),\n",
    "    (14.569199562072754, 36.056400299072266),\n",
    "    (14.366399765014648, 36.07870101928711),\n",
    "    (14.406900405883789, 36.07870101928711),\n",
    "    (14.447500228881836, 36.07870101928711),\n",
    "    (14.48799991607666, 36.07870101928711),\n",
    "    (14.528599739074707, 36.07870101928711),\n",
    "    (14.569199562072754, 36.07870101928711),\n",
    "    (14.528599739074707, 35.945098876953125),\n",
    "    (14.406900405883789, 36.03409957885742),\n",
    "    (14.406900405883789, 36.056400299072266),\n",
    "    (14.406900405883789, 36.01190185546875),\n",
    "    (14.366399765014648, 36.03409957885742),\n",
    "    (14.366399765014648, 36.056400299072266),\n",
    "    (14.48799991607666, 35.96730041503906),\n",
    "    (14.406900405883789, 35.989601135253906,),\n",
    "    (14.366399765014648, 36.01190185546875),\n",
    "    (14.447500228881836, 35.96730041503906)\n",
    "]\n",
    "\n",
    "# Loop through each pair of coordinates along with an index\n",
    "for index, (lon, lat) in enumerate(coordinates_inside, start=1):\n",
    "    # Print the current pair being processed\n",
    "    print(f\"Processing pair {index}: (lat: {lat}, lon: {lon})\")\n",
    "\n",
    "    # Filter the DataFrame for the exact coordinates\n",
    "    df_point = final_df[(final_df['lat'] == lat) & (final_df['lon'] == lon)]\n",
    "\n",
    "    # Check if any data points were found for the current pair\n",
    "    if df_point.empty:\n",
    "        print(f\"No data points found for (lat: {lat}, lon: {lon})\")\n",
    "    else:\n",
    "        print(f\"Found {df_point.shape[0]} data points for (lat: {lat}, lon: {lon})\")\n",
    "\n",
    "    # Format the latitude and longitude to remove decimal points and replace with underscores\n",
    "    lat_formatted = str(lat).replace('.', '_')\n",
    "    lon_formatted = str(lon).replace('.', '_')\n",
    "\n",
    "    # Create a unique filename for each pair of coordinates\n",
    "    filename = f\"{index}_{lat_formatted}_{lon_formatted}.csv\"\n",
    "    filepath = os.path.join(save_directory, filename)\n",
    "\n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    df_point.to_csv(filepath, index=False)\n",
    "\n",
    "    # Confirm that the file has been saved\n",
    "    print(f\"Saved data for (lat: {lat}, lon: {lon}) as {filename}\\n\")\n",
    "\n",
    "print(\"All data processing complete.\")\n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation, Training & Evaluation\n",
    "\n",
    "- **Model Storage**: Set up a directory to save the trained models.\n",
    "- **Data Preparation**: List and load all coordinate-specific CSV files that were prepared earlier.\n",
    "- **Feature Selection**: For each dataset, 'u' and 'v' values are chosen as features with 'u' set as the target variable.\n",
    "- **Time Series Generation**: Construct time series generators, utilizing the last 72 hours (3 days) of data to predict the 'u' value for the next hour. This historical data frame helps the model understand temporal patterns.\n",
    "- **Early Stopping**: Integrate early stopping to halt training when the validation loss stops improving, preventing overfitting and saving computational resources.\n",
    "- **Model Architecture**: Build the LSTM (Long Short-Term Memory) network. The model architecture includes several layers with dropout rates to manage overfitting.\n",
    "- **Training Process**: Train the model on the generated time series data, monitoring both training and validation metrics.\n",
    "- **Prediction and Evaluation**:\n",
    "    - **Prediction**: After training, apply the model to the test set to forecast future 'u' values.\n",
    "    - **Evaluation Metrics**: Use three key metrics to evaluate model performance:\n",
    "        - **Mean Squared Error (MSE)**: Represents the average of the squares of the errors between actual and predicted values. Lower values indicate better performance.\n",
    "        - **Mean Absolute Error (MAE)**: Measures the average magnitude of errors between pairs of actual and predicted values, without considering their direction. Lower MAE values signify better accuracy.\n",
    "        - **Root Mean Squared Error (RMSE)**: The square root of MSE, providing error metrics in the same units as the data, making interpretation straightforward. Lower RMSE values denote better fit.\n",
    "    - **Visualization**: Plot actual vs. predicted 'u' values to visually assess the model's prediction accuracy and understand its real-world applicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for saving models \n",
    "model_save_directory = \"Data/saved_models/models_u\"\n",
    "os.makedirs(model_save_directory, exist_ok=True)\n",
    "\n",
    "# Function to extract the numerical index from the filename\n",
    "def extract_index(filename):\n",
    "    # Index is always before the first underscore\n",
    "    index_part = filename.split(\"_\")[0]\n",
    "    try:\n",
    "        return int(index_part)\n",
    "    except ValueError:\n",
    "        # In case of any error, return a large number to sort this file at the end\n",
    "        return float('inf')\n",
    "\n",
    "# List all CSV files in the directory and sort them numerically based on the index\n",
    "csv_files = sorted([f for f in os.listdir(save_directory) if f.endswith('.csv')], key=extract_index)\n",
    "\n",
    "print(csv_files+\"\\n\")\n",
    "print(\"=\"*175)\n",
    "\n",
    "# Set up early stopping callback to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               patience=8, \n",
    "                               mode='min', \n",
    "                               restore_best_weights=True)\n",
    "\n",
    "# Loop through each CSV file in the directory\n",
    "for file in csv_files:\n",
    "    print(\"=\"*175)\n",
    "    print(f\"Processing file: {file}\\n\")\n",
    "\n",
    "    # Load the dataset from the CSV file\n",
    "    df = pd.read_csv(os.path.join(save_directory, file))\n",
    "\n",
    "    # Convert the 'time' column to datetime object and set it as the index \n",
    "    df['time'] = pd.to_datetime(df['time'], infer_datetime_format=True)\n",
    "    df.set_index('time', inplace=True)\n",
    "\n",
    "    # Select 'u' and 'v' columns as input features \n",
    "    df_input = df[['u', 'v']]\n",
    "    features = df_input.to_numpy()\n",
    "\n",
    "    # Set the 'u' column as the target variable \n",
    "    target = df_input['u'].values\n",
    "\n",
    "    # Splitting Data into Train, Validation, and Test sets (70-15-15 split)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=123, shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=123, shuffle=False)\n",
    "\n",
    "    # Time series generator parameters\n",
    "    win_length = 72 # 3 Days\n",
    "    batch_size = 64 # 64 samples per batch\n",
    "    num_features = 2 # 'u' and 'v' columns\n",
    "\n",
    "    # Creating the time series generators for the training, validation, and test sets\n",
    "    train_generator = TimeseriesGenerator(X_train, y_train, length=win_length, sampling_rate=1, batch_size=batch_size)\n",
    "    val_generator = TimeseriesGenerator(X_val, y_val, length=win_length, sampling_rate=1, batch_size=batch_size)\n",
    "    test_generator = TimeseriesGenerator(X_test, y_test, length=win_length, sampling_rate=1, batch_size=batch_size)\n",
    "\n",
    "    # Building the LSTM Model Architecture\n",
    "    model = Sequential([\n",
    "        LSTM(256, input_shape=(win_length, num_features), return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compile the model \n",
    "    model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.001), metrics=[MeanAbsoluteError()])\n",
    "\n",
    "    # Define ModelCheckpoint callback to save the best model during training\n",
    "    model_checkpoint = ModelCheckpoint(filepath=f\"{model_save_directory}/{file[:-4]}\",\n",
    "                                       save_best_only=True, \n",
    "                                       monitor='val_loss', \n",
    "                                       mode='min', \n",
    "                                       save_format='tf',\n",
    "                                       verbose=0)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    history = model.fit(train_generator, epochs=100, validation_data=val_generator, shuffle=False, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    # Determine the epoch number at best performance\n",
    "    best_epoch = early_stopping.stopped_epoch - early_stopping.patience\n",
    "\n",
    "    # Print the best epoch number and the best validation loss achieved\n",
    "    print(\"=\"*175)\n",
    "    print(f\"Training stopped at epoch: {best_epoch + 1}\")\n",
    "    print(f\"Best validation loss achieved at epoch: {best_epoch + 1}\\n\")\n",
    "\n",
    "    # Plot training and validation loss values \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Plot training and validation mean absolute error values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mean_absolute_error'], label='Train MAE')\n",
    "    plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
    "    plt.title('Model Mean Absolute Error')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Display the plots \n",
    "    plt.suptitle(f'Results for {file[:-4]}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Generate predictions for the test data \n",
    "    predictions = model.predict(test_generator)\n",
    "    test_indexes = df.index[-len(predictions):]\n",
    "\n",
    "    # Prepare the final DataFrame for plotting and analysis \n",
    "    df_final = pd.DataFrame(df.loc[test_indexes])\n",
    "    df_final['u_pred'] = predictions.flatten()\n",
    "\n",
    "    # Calculate evaluation metrics  \n",
    "    mse = mean_squared_error(df_final['u'], df_final['u_pred'])\n",
    "    mae = mean_absolute_error(df_final['u'], df_final['u_pred'])\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f'\\nTest set Evaluation metrics for file: {file[:-4]}')\n",
    "    print(f'Mean Squared Error (MSE): {mse}')\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}\\n')\n",
    "\n",
    "    # Plot Actual vs. Predicted 'u' Values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df_final.index, df_final['u'], label='Actual', color='blue', marker='o', linestyle='-', markersize=5)\n",
    "    plt.plot(df_final.index, df_final['u_pred'], label='Predicted', color='red', marker='x', linestyle='--', markersize=5)\n",
    "    plt.title(f'Actual vs. Predicted \"u\" Values for {file[:-4]}')  \n",
    "    plt.xlabel('Date') \n",
    "    plt.ylabel('u')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*175)\n",
    "    print(f\"Model tarining & evaluation completed for file: {file}\")\n",
    "    print(\"=\"*175, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Real-life Predictions from Data (Simulation)\n",
    "This section outlines the process of making predictions based on real-life data collected over a span of three days. The goal is to use data from the previous three days to simulate a prediction of the sea surface currents for the 4th of August, demonstrating the application in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract 3 Days of Inputs for Each Coordinate Pair\n",
    "\n",
    "- **Data Loading**: Retrieve sea surface current data from August 1, 2023, to August 3, 2023, and load it.\n",
    "- **Dataframe Conversion**: Transform the loaded NetCDF data into a pandas DataFrame.\n",
    "- **Data Cleaning**: Remove unnecessary columns.\n",
    "- **Data Organization**: Set up a directory specifically for storing the processed data files.\n",
    "- **Data Processing**:\n",
    "    - For each predefined pair of coordinates, isolate the data corresponding to that location.\n",
    "    - Ensure the data is in order to maintain the sequence integrity.\n",
    "    - Apply cubic spline interpolation to fill in any missing 'u' and 'v' values, ensuring a continuous dataset.\n",
    "- **File Saving**: Format latitude and longitude information for compatibility with filenames and save the individual data frames as seperate CSV files. Each file is uniquely named to correspond with its geographic location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (from August 1, 2023 till August 3, 2023) \n",
    "input_netcdf_path = \"Data/model_IO/3_day_input_SSC_Data.nc\"\n",
    "dataset = xr.open_dataset(input_netcdf_path)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame and reset the index\n",
    "df = dataset.to_dataframe().reset_index()\n",
    "\n",
    "# Drop columns from the DataFrame that are not needed\n",
    "df = df.drop(columns=['stdu', 'stdv', 'cov', 'velo', 'head'])\n",
    "\n",
    "# Directory to save the files \n",
    "input_data_directory = \"Data/3_day_input_data_frames\"\n",
    "os.makedirs(input_data_directory, exist_ok=True)\n",
    "\n",
    "# Loop through each pair of coordinates \n",
    "for index, (lon, lat) in enumerate(coordinates_inside, start=1):\n",
    "    # Extract data for the specific coordinates and copy it\n",
    "    df_specific = df[(df['lat'] == lat) & (df['lon'] == lon)].copy()\n",
    "\n",
    "    # Ensure the DataFrame is sorted by time \n",
    "    df_specific.sort_values('time', inplace=True)\n",
    "\n",
    "    # Interpolate NaN values using cubic spline interpolation\n",
    "    df_specific.loc[:, ['u', 'v']] = df_specific[['u', 'v']].interpolate(method='spline', order=3)\n",
    "\n",
    "    # After interpolation, if there are still NaNs at the beginning or the end, fill them in using the nearest value\n",
    "    df_specific.loc[:, ['u', 'v']] = df_specific[['u', 'v']].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # Replace decimal points with underscores in lat and lon for the filename\n",
    "    lat_str = str(lat).replace('.', '_')\n",
    "    lon_str = str(lon).replace('.', '_')\n",
    "\n",
    "    # Save to CSV file with a unique filename\n",
    "    filename = f\"{index}_{lat_str}_{lon_str}_3_day.csv\"\n",
    "    filepath = os.path.join(input_data_directory, filename)\n",
    "    df_specific.to_csv(filepath, index=False)\n",
    "    print(f\"Saved 3-day data for (lat: {lat}, lon: {lon}) as {filename}\")\n",
    "    \n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract 1 day of actaul data for each coordinate pair to use for comparison \n",
    "\n",
    "- **Data Loading**: Access actual sea surface current data for August 4th, 2023, to compare against the predictions.\n",
    "- **Dataframe Conversion**: Transform the NetCDF data into a pandas DataFrame.\n",
    "- **Data Cleaning**: Remove unnecessary columns.\n",
    "- **File Organization**: Establish a directory specifically for storing actual data files.\n",
    "- **Data Processing**:\n",
    "    - For each predefined pair of coordinates, isolate the data corresponding to that location.\n",
    "    - Sort this data to maintain the correct time sequence.\n",
    "    - Fill in missing 'u' and 'v' data points using cubic spline interpolation.\n",
    "- **File Saving**: Adjust latitude and longitude information for filename compatibility and save the processed data frames as seperate CSV files. Each file is distinctly named according to its geographic location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for the next day (August 4th, 2023) \n",
    "actual_netcdf_path = \"Data/model_IO/1_day_actual_4th_SSC_Data.nc\"\n",
    "actual_dataset = xr.open_dataset(actual_netcdf_path)\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame and reset index\n",
    "actual_df = actual_dataset.to_dataframe().reset_index()\n",
    "\n",
    "# Drop columns from the DataFrame that are not needed\n",
    "actual_df = actual_df.drop(columns=['stdu', 'stdv', 'cov', 'velo', 'head'])\n",
    "\n",
    "# Directory to save the files\n",
    "actual_data_directory = \"Data/1_day_actual_data_frames\"\n",
    "os.makedirs(actual_data_directory, exist_ok=True)\n",
    "\n",
    "# Loop through each pair of coordinates\n",
    "for index, (lon, lat) in enumerate(coordinates_inside, start=1):\n",
    "    # Extract data for the specific coordinates and copy it\n",
    "    actual_df_specific = actual_df[(actual_df['lat'] == lat) & (actual_df['lon'] == lon)].copy()\n",
    "\n",
    "    # Ensure the DataFrame is sorted by time\n",
    "    actual_df_specific.sort_values('time', inplace=True)\n",
    "\n",
    "    # Interpolate NaN values using cubic spline interpolation\n",
    "    actual_df_specific.loc[:, ['u', 'v']] = actual_df_specific[['u', 'v']].interpolate(method='spline', order=3)\n",
    "\n",
    "    # After interpolation, if there are still NaNs at the beginning or the end, fill them in using the nearest value\n",
    "    actual_df_specific.loc[:, ['u', 'v']] = actual_df_specific[['u', 'v']].fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # Replace decimal points with underscores in lat and lon for the filename\n",
    "    lat_str = str(lat).replace('.', '_')\n",
    "    lon_str = str(lon).replace('.', '_')\n",
    "\n",
    "    # Save to CSV file with a unique filename\n",
    "    filename = f\"{index}_{lat_str}_{lon_str}_1_day.csv\"\n",
    "    filepath = os.path.join(actual_data_directory, filename)\n",
    "    actual_df_specific.to_csv(filepath, index=False)\n",
    "    print(f\"Saved 1-day actual data for (lat: {lat}, lon: {lon}) as {filename}\")\n",
    "\n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Prediction for 24 hours\n",
    "\n",
    "This section performs a series of rolling predictions, for a total of 24 hours, utilizing the previously trained models. \n",
    "\n",
    "- **Prediction Initialization**: Begin with an empty list to store outcomes from all models.\n",
    "- **Prediction Span**: Set the forecast length to 24 hours (1 day).\n",
    "- **Model Access**: Locate all saved LSTM models from the designated storage directory.\n",
    "- **Iterative Forecasting**: For each saved model:\n",
    "    - Retrieve the model's unique identifier.\n",
    "    - Load the LSTM model.\n",
    "    - Prepare input data derived from the preceding three days corresponding to each geographical location.\n",
    "    - Execute sequential predictions where each hourly prediction informs the next, mirroring real-world forecasting scenarios. Specifically:\n",
    "        - Utilize the model to forecast the next 'u' value based on the last 72 hours (3 days) of data.\n",
    "        - Update the input feature set for the subsequent prediction by appending the newly predicted 'u' value while shifting the dataset forward by one hour, ensuring the model consistently receives fresh data reflecting the most recent conditions.\n",
    "        - Importantly, while 'u' values are iteratively forecasted and updated, the 'v' values are directly drawn from the actual data for the corresponding hour, maintaining the model's contextual relevance to evolving environmental conditions.\n",
    "- **Data Synchronization**: Align each prediction with actual data to evaluate the model's performance accurately.\n",
    "    - Preprocess real data to match the model's input structure.\n",
    "    - Record every hourly forecast alongside actual measurements for comparison.\n",
    "- **Performance Metrics**: For each model, compute and report key metrics:\n",
    "    - **MAE (Mean Absolute Error)**: Reflects the average magnitude of errors between predicted and actual values, irrespective of direction.\n",
    "    - **MSE (Mean Squared Error)**: Emphasizes larger errors by squaring them, thus penalizing more significant discrepancies more severely.\n",
    "    - **RMSE (Root Mean Squared Error)**: Provides error magnitude in the same units as the predicted values.\n",
    "- **Result Compilation**: Aggregate all hourly predictions across all models and locations into a single DataFrame, creating a comprehensive overview of the forecasting performance.\n",
    "- **Save Predictions**: Store the combined predictions as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to collect the results of all predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Number of hours to predict into the future\n",
    "n_future_hours = 24  \n",
    "\n",
    "# Specify the directory where saved models are stored\n",
    "model_directory = \"Data/saved_models/models_u\"\n",
    "\n",
    "# Retrieve paths for all saved models within the specified directory \n",
    "model_paths = [os.path.join(model_directory, filename).replace(\"\\\\\", \"/\") for filename in os.listdir(model_directory)]\n",
    "\n",
    "# Iterate over each model to perform predictions\n",
    "for model_path in model_paths:\n",
    "    # Extract model identifier from the file name\n",
    "    model_id = os.path.basename(model_path).split('.')[0]  \n",
    "    \n",
    "    print(\"=\"*175)\n",
    "    print(f\"Processing model: {model_id}\")\n",
    "    print(\"=\"*175, \"\\n\")\n",
    "\n",
    "    # Load the saved model from its path\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Define paths for input data and actual comparison data\n",
    "    input_data_path = os.path.join(input_data_directory, f\"{model_id}_3_day.csv\").replace(\"\\\\\", \"/\")\n",
    "    actual_data_path = os.path.join(actual_data_directory, f\"{model_id}_1_day.csv\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "    # Load and preprocess input data from CSV file\n",
    "    input_data_df = pd.read_csv(input_data_path)\n",
    "    input_data_df['time'] = pd.to_datetime(input_data_df['time'])\n",
    "    features = input_data_df[['u', 'v']].values[-win_length:].reshape(1, win_length, num_features)\n",
    "\n",
    "    # Initialize a list to store predictions for each hour\n",
    "    predictions_u = []\n",
    "    # Perform rolling predictions for the defined number of future hours\n",
    "    for i in range(n_future_hours):\n",
    "        # Predict the next value using the model\n",
    "        current_prediction_u = model.predict(features, verbose=0)[0, 0]\n",
    "        # Append the prediction to the list\n",
    "        predictions_u.append(current_prediction_u)\n",
    "        # Update the features for the next prediction step\n",
    "        next_v_values = np.roll(features[0, :, 1], -1)\n",
    "        # Use the most recent 'v' value\n",
    "        next_v_values[-1] = features[0, -1, 1]  \n",
    "        features = np.column_stack((np.roll(features[0, :, 0], -1), next_v_values)).reshape(1, win_length, num_features)\n",
    "        # Set the last 'u' value to the current prediction\n",
    "        features[0, -1, 0] = current_prediction_u  \n",
    "\n",
    "    # Load and preprocess actual data for comparison\n",
    "    actual_data_df = pd.read_csv(actual_data_path)\n",
    "    actual_data_df['time'] = pd.to_datetime(actual_data_df['time'])\n",
    "    actual_u_values = actual_data_df['u'].values[:n_future_hours]\n",
    "    # Use the first row's lat and lon for location\n",
    "    lat, lon = actual_data_df[['lat', 'lon']].values[0]  \n",
    "\n",
    "    # Compile all predictions with their corresponding timestamps and locations\n",
    "    for i, predicted_u in enumerate(predictions_u):\n",
    "        all_predictions.append([actual_data_df['time'].iloc[i], lat, lon, predicted_u])\n",
    "\n",
    "    # Calculate evaluation metrics \n",
    "    mae = mean_absolute_error(actual_u_values, predictions_u)\n",
    "    mse = mean_squared_error(actual_u_values, predictions_u)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Display calculated metrics\n",
    "    print(f\"Metrics for model {model_id}:\\nMAE = {mae}\\nMSE = {mse}\\nRMSE = {rmse}\\n\")\n",
    "\n",
    "print(\"=\"*175)\n",
    "\n",
    "# Convert the list of all predictions into a DataFrame\n",
    "predictions_df = pd.DataFrame(all_predictions, columns=['time', 'lat', 'lon', 'u'])\n",
    "\n",
    "# Save the compiled predictions to a CSV file\n",
    "predictions_df.to_csv(\"Data/model_IO/merged_predictions_u.csv\", index=False)\n",
    "print(\"Saved merged predictions to 'Data/model_IO/merged_predictions_u.csv'\")\n",
    "print(\"=\"*175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
