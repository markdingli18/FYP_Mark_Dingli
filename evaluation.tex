\graphicspath{{content/chapters/4_evaluation/evaluation_figures}}
\newtheorem{hypothesis}{Hypothesis}[chapter]

\chapter{Evaluation}
\label{chp:evaluation}

This chapter provides a detailed overview and a comprehensive explanation of the results obtained through the evaluation strategy outlined in Section~\ref{sec:3.5}. The primary objectives are to ascertain which model, \acrshort{lstm} or \acrshort{gru}, demonstrates superior performance, and to evaluate the similarity of the Lagrangian simulations generated using these models' predictions. As previously noted, the framework was executed on two specific dates—August 4\textsuperscript{th} and November 4\textsuperscript{th}, 2023—to gauge the models' consistency and reliability under varying seasonal conditions, offering a thorough analysis of their performance across different environmental dynamics. The chapter is structured into two sections: Section~\ref{sec:4.1} delves into the analysis of average error metrics to discern which model performed best. Section~\ref{sec:4.2} focuses on a geospatial analysis to investigate the impact of to find out if the locations and the amount of data play a role in the predictions, while also briefly comparing the merged predictions with the Lagrangian simulations. This approach ensures a thorough evaluation of the models' effectiveness and their applicability in real-world scenarios.

\section{LSTM vs GRU}
\label{sec:4.1}

In the initial experiment, we assessed the accuracy of the models in predicting \acrshort{ssc} velocities by comparing the predicted results against actual historical values using three key error metrics:
\begin{itemize}
    \item \textbf{\acrshort{mae}:} This metric computes the average absolute difference between actual and predicted values across the dataset. It quantifies the typical magnitude of the prediction errors without considering their direction, providing a clear measure of average error.
    \item \textbf{\acrshort{mse}:} This represents the average of the squares of the differences between actual and predicted values. It accentuates larger errors more significantly than smaller ones by squaring the differences, highlighting impactful prediction discrepancies.
    \item \textbf{\acrshort{rmse}:} Calculated as the square root of the \acrshort{mse}, it measures the standard deviation of residuals, offering a scale-sensitive accuracy measure. It provides an indication of the typical magnitude of prediction errors in the same units as the data.
\end{itemize}

\subsection{Error Metrics Results}
\label{subsec:4.1.1}

The average error metrics for the 24-hour rolling predictions from all 37 models on the 4\textsuperscript{th} of August are presented in the tables below:\newline

\begin{table}[H]
\centering
\begin{minipage}{0.5\textwidth}
    \centering
    \caption{\acrshort{lstm} \textit{'u'} average error metrics (August).\label{tab:4.1}}
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14126 & 0.22658 & 0.05830 \\
        \textbf{MSE} & 0.11693 & 0.51328 & 0.01012 \\
        \textbf{RMSE} & 0.17957 & 0.29100 & 0.05289 \\
        \hline
    \end{tblr}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
    \centering
    \caption{\acrshort{lstm} \textit{'v'} average error metrics (August).\label{tab:4.2}}
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14370 & 0.13416 & 0.14072 \\
        \textbf{MSE} & 0.06405 & 0.10900 & 0.07281 \\
        \textbf{RMSE} & 0.18300 & 0.17483 & 0.21160 \\
        \hline
    \end{tblr}
\end{minipage}
\end{table}

\begin{table}[H]
\centering
\begin{minipage}{0.5\textwidth}
    \centering
    \caption{\acrshort{gru} \textit{'u'} average error metrics (August).\label{tab:4.3}}
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14849 & 0.22171 & 0.06700 \\
        \textbf{MSE} & 0.11589 & 0.50339 & 0.01603 \\
        \textbf{RMSE} & 0.18693 & 0.28451 & 0.07046 \\
        \hline
    \end{tblr}
\end{minipage}\hfill
\begin{minipage}{0.5\textwidth}
    \centering
    \caption{\acrshort{gru} \textit{'v'} average error metrics (August).\label{tab:4.4}}
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14472 & 0.13763 & 0.14908 \\
        \textbf{MSE} & 0.06558 & 0.11215 & 0.06614 \\
        \textbf{RMSE} & 0.18376 & 0.17936 & 0.20195 \\
        \hline
    \end{tblr}
\end{minipage}
\end{table}

The analysis of these results reveals insightful differences in model performance. For the \textit{'u'} component, \acrshort{lstm} models demonstrate slightly lower \acrshort{mae} and \acrshort{rmse}, indicating better average accuracy and consistency, although \acrshort{gru} models show a marginally lower \acrshort{mse}. Conversely, for the \textit{'v'} component, both models perform similarly with minimal variations across all metrics, which suggests a near-equivalent capability in handling this type of prediction. Further examination of the variability through Standard Deviation and \acrshort{iqr} metrics shows that \acrshort{lstm} models have a lower standard deviation in the \textit{'v'} component predictions, suggesting more consistent performance relative to \acrshort{gru}. Additionally, the smaller \acrshort{iqr} for \acrshort{lstm} in both components implies that its predictions are more tightly clustered around the median, indicating less variability and more reliability. While both models performed well, \acrshort{lstm} offered a marginally better performance, particularly for the \textit{'u'} component, establishing it as the preferable model. 

On the other hand, the results for the 24-hour rolling predictions for all 37 models on the 4\textsuperscript{th} of November are detailed below:

\begin{table}[H]
\centering
\begin{minipage}{0.47\textwidth}
    \centering
    \caption{\acrshort{lstm} \textit{'u'} average error metrics (November).\label{tab:4.5}}
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 1.03141 & 2.11801 & 0.29923 \\
        \textbf{MSE} & 14.76509 & 40.75769 & 0.24335 \\
        \textbf{RMSE} & 1.63416 & 3.47773 & 0.39745 \\
        \hline
    \end{tblr}
\end{minipage}\hfill
\begin{minipage}{0.47\textwidth}
    \centering
    \caption{\acrshort{lstm} \textit{'v'} average error metrics (November).\label{tab:4.6}}
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 2.62164 & 5.50710 & 0.50617 \\
        \textbf{MSE} & 97.85786 & 253.42893 & 0.86051 \\
        \textbf{RMSE} & 4.23877 & 8.93816 & 0.84444 \\
        \hline
    \end{tblr}
\end{minipage}
\end{table}

\begin{table}[H]
\centering
\begin{minipage}{0.47\textwidth}
    \centering
    \caption{\acrshort{gru} \textit{'u'} average error metrics (November).\label{tab:4.7}}
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 1.05110 & 2.11189 & 0.56820 \\
        \textbf{MSE} & 14.77234 & 40.77340 & 0.46635 \\
        \textbf{RMSE} & 1.65105 & 3.47079 & 0.58636 \\
        \hline
    \end{tblr}
\end{minipage}\hfill
\begin{minipage}{0.47\textwidth}
    \centering
    \caption{\acrshort{gru} \textit{'v'} average error metrics (November).\label{tab:4.8}}
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 2.65316 & 5.50156 & 0.53727 \\
        \textbf{MSE} & 97.98028 & 253.62271 & 1.14379 \\
        \textbf{RMSE} & 4.26801 & 8.93109 & 0.99123 \\
        \hline
    \end{tblr}
\end{minipage}
\end{table}

The analysis reveals several insights. For the \textit{'u'} component, both models display relatively high \acrshort{mae}, \acrshort{mse}, and \acrshort{rmse}, with \acrshort{lstm} showing lower metrics. The high standard deviations observed for both models suggest a significant presence of outliers, indicating some predictions were inaccurate. This is evident in the \acrshort{gru} \textit{'u'} component where the \acrshort{iqr} is higher, suggesting a broader spread compared to \acrshort{lstm}, pointing to more frequent outliers in the \acrshort{gru} model. In contrast, the '\textit{v'} component shows considerably higher error values for both models, with \acrshort{gru} again having higher values across all metrics. The standard deviations and \acrshort{iqr} values are significantly larger in the \textit{'v'} component for both models, reinforcing the presence of outliers and indicating that predictions for the \textit{'v'} component were generally less accurate and more variable. Overall, the \acrshort{lstm} model performs slightly better than the GRU model, particularly in the \textit{'v'} component, as evidenced by the lower error metrics and narrower \acrshort{iqr}. Therefore, the more consistent performance of \acrshort{lstm} across both components makes it the better model overall. 

\subsection{Error Metrics Discussion of Results}
\label{subsec:4.1.2}

The analysis highlights that predictions for the \textit{'u'} component (east-west velocity) were generally more accurate than for the \textit{'v'} component (north-south velocity). This discrepancy can be attributed to the alignment of radar systems, which are aligned in a north-south orientation, as depicted in Figure~\ref{fig_2.1}. This alignment potentially impacts the accuracy of \textit{'v'} component readings and inhibits the radar's ability to capture detailed north-south data, consequently leading to less accurate predictions for the \textit{'v'} component. Moreover, the first experiment conducted on the 4\textsuperscript{th} of August exhibited notably better results, characterised by lower error values and fewer outliers when compared to the subsequent November evaluation. This improvement is likely due to the August data being in close proximity to the final sequences of the test dataset, potentially leading to the models being better tuned to these conditions. Furthermore, the process of rolling forecasting, which bases predictions on preceding outputs, may lead to inaccuracies, particularly when initial predictions are derived from interpolated data. This method could inherently propagate errors, especially under conditions of notable missing data as discussed in Sub-section~\ref{subsec:3.3.3}.

Such findings emphasise the necessity of considering temporal proximity and data integrity when assessing model performance. These phenomena and their implications on model performance will be explored further in the next section.

\section{Geospatial Analysis}
\label{sec:4.2}

In this section, we extended the analysis inspired by the insights garnered from the heat map depicted in Figure~\ref{sec:3.5}. Our objectives were multifaceted; we aimed to determine whether an increased volume of data correlates with enhanced predictive accuracy, whether data points closer to the coast—typically characterised by less data-yield poorer performance, and whether the time of year and seasonality of the data impact the results. The primary goal was to assess whether the geographical location of data points influences the accuracy of the predictive models. Additionally, we sought to compare the final Lagrangian simulations generated by both the \acrshort{lstm} and \acrshort{gru} models to evaluate their similarities in modelling sea surface debris.

\subsection{The Hypothesis}
\label{subsec:4.2.1}

As highlighted in Sub-sections~\ref{subsec:3.3.1} and~\ref{subsec:3.5.2}, the dataset contains a significant number of \acrshort{nan}s, with data points closer to the coast having more \acrshort{nan}s present, as evidenced in Figure~\ref{fig_3.5}. This is corroborated by Figure~\ref{fig_4.1}, which illustrates how some data points have significantly less data available. Figure~\ref{fig_3.5} is instrumental in demonstrating the correlation between model performance and geographic location, thereby paving the way for a focused analysis on the impact of data availability at specific locations. Based on these observations, we formulated a hypothesis:
\begin{hypothesis}
A plausible hypothesis could propose that data points near the coast exhibit reduced data availability due to environmental and technical challenges that interfere with radar performance. This scarcity of data consequently impairs the accuracy of predictive models for \acrshort{ssc} velocities near the coast. Specifically, it suggests that models predicting \acrshort{ssc} at coastal locations perform less effectively compared to those further offshore, where radar data tends to be more complete.
\label{hyp}
\end{hypothesis}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.72\textwidth,keepaspectratio]{histogram_of_data_points_by_location_number.pdf}
    \caption[Histogram of data points by location number.]{-- Histogram of data points by location number.\label{fig_4.1}}
\end{figure}

The following Subsections will analyse hypothesis~\ref{hyp}, employing a comparative analysis with the heat map presented in Figure~\ref{fig_3.5} to validate or refute our assumption regarding the influence of geographical location on model accuracy.

\subsection{Heat maps Results and Analysis}
\label{subsec:4.2.2}

As outlined in Sub-section~\ref{subsec:3.5.2}, we employed heat maps to geospatially analyse the performance of our models, focusing on the \acrshort{mae} across all 37 models. These visualisations are crucial for testing the validity of hypothesis~\ref{hyp} regarding the impact of data availability on predictive accuracy near coastal areas. Heat maps for the \acrshort{mae} error metrics were produced for both the \textit{'u'} and \textit{'v'} components of both \acrshort{lstm} and \acrshort{gru} models, covering both the August 4\textsuperscript{th} and November 4\textsuperscript{th}, 2023 predictions. To enhance the clarity of these visualisations and minimise the influence of outliers, we applied a clipping method at the 95\textsuperscript{th} percentile of the data. This method effectively limited the range of data considered for colour scaling in the heat maps, allowing for more nuanced visual comparisons between most data points by excluding extreme outliers. The resultant heat maps for August 4\textsuperscript{th} are displayed as follows:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.78\textwidth,keepaspectratio]{u_component_MAE_heatmaps_(Aug).pdf}
    \caption[\textit{'u'} component MAE heat maps (August).]{-- \textit{'u'} component \acrshort{mae} heat maps (August).\label{fig_4.2}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.78\textwidth,keepaspectratio]{v_component_MAE_heatmaps_(Aug).pdf}
    \caption[\textit{'v'} component MAE heat maps (August).]{-- \textit{'v'} component \acrshort{mae} heat maps (August).\label{fig_4.3}}
\end{figure}

\noindent The heat maps for November 4\textsuperscript{th} are detailed in the subsequent figures:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.78\textwidth,keepaspectratio]{u_component_MAE_heatmaps_(Nov).pdf}
    \caption[\textit{'u'} component MAE heat maps (November).]{-- \textit{'u'} component \acrshort{mae} heat maps (November).\label{fig_4.4}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.78\textwidth,keepaspectratio]{v_component_MAE_heatmaps_(Nov).pdf}
    \caption[\textit{'v'} component MAE heat maps (November).]{-- \textit{'v'} component \acrshort{mae} heat maps (November).\label{fig_4.5}}
\end{figure}

The hypothesis~\ref{hyp} is nuanced by the evidence presented in the \acrshort{mae} heat maps. These visualisations clarify that while models near the coast generally perform worse, presumably due to less data being available, there are notable exceptions where coastal predictions maintained good accuracy. This observation challenges the assumption that greater data volume directly correlates with higher predictive accuracy. Instead, it suggests that the models' capabilities to handle noise and extract meaningful patterns from sparse data can significantly impact their effectiveness. Furthermore, the differential accuracy between the \textit{'u'} and \textit{'v'} components highlights the complexity of environmental factors and model sensitivities, which contribute to a diverse spectrum of outcomes that are not solely determined by the amount of data available.

Dividing the analysis between the August and November predictions reveals a stark contrast; predictions for August are markedly more accurate, underscoring the importance of the temporal proximity of training data to the predictive period and the impact of seasonal variations. The outliers observed on August 4\textsuperscript{th} are not consistent with those on November 4\textsuperscript{th}, highlighting the temporal variability of missing data and its non-uniform impact across different points and times. It becomes evident that no consistent pattern exists, thus challenging any definitive conclusions. While certain models excel in predicting the \textit{'u'} component, their performance diminishes when tasked with the \textit{'v'} component. These observations attest to the intricate dynamics at play in predictive modelling, where factors like the models' ability to manage noise and the inherent directional properties of data lead to outcomes where less data does not always result in less accuracy. Therefore, these findings not only challenge but effectively disprove hypothesis~\ref{hyp}.

\subsection{Comparison of Lagrangian Simulations}
\label{subsec:4.2.3}

In the final component of the evaluation framework, we assessed the performance similarities between the \acrshort{lstm} and \acrshort{gru} models by analysing their centroids, spreads, and skewness within their respective Lagrangian simulation outputs. The findings for August 4\textsuperscript{th} are depicted in Figure~\ref{fig_4.6}:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth,keepaspectratio]{LSTM_vs_GRU_comparison_on_lagrangian_simulations_(august).pdf}
    \caption[LSTM vs GRU comparison on Lagrangian simulations (August).]{-- \acrshort{lstm} vs \acrshort{gru} comparison on Lagrangian simulations (August).\label{fig_4.6}}
\end{figure}

For the 4\textsuperscript{th} of August, the average centroid distances for the \acrshort{lstm} and \acrshort{gru} models were around 2 km, suggesting both models achieved different geographical accuracy. The standard deviation of these distances was relatively low at 1.54 km, indicating consistent clustering near the centroids. However, the \acrshort{gru} model demonstrated a more compact spread of 0.54 km compared to the \acrshort{lstm}'s 1.81 km, suggesting \acrshort{gru}'s predictions were more tightly grouped. The skewness metrics showed a mild eastward and southward bias in \acrshort{lstm}'s predictions, whereas \acrshort{gru} exhibited a stronger westward bias and a slight northward tendency, highlighting directional tendencies in their prediction patterns.\newline

\noindent On the contrary, the results for the 4\textsuperscript{th} of November are as follows:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth,keepaspectratio]{LSTM_vs_GRU_comparison_on_lagrangian_simulations_(november).pdf}
    \caption[LSTM vs GRU comparison on Lagrangian simulations (November).]{-- LSTM vs GRU comparison on Lagrangian simulations (November).\label{fig_4.7}}
\end{figure}

Conversely, the results from the 4\textsuperscript{th} of November showed improvements in clustering, with mean and median centroid distances reduced to approximately 1.63 km and 1.91 km respectively. This reduction, coupled with a decreased standard deviation of 0.70 km, suggests enhanced prediction accuracy and consistency for this period. Interestingly, \acrshort{lstm} showed a more consistent spread of 1.60 km, whereas \acrshort{gru}’s predictions were more dispersed, with a spread of 2.58 km. Skewness values also shifted, indicating changes in predictive behaviour that might be influenced by different environmental conditions or model sensitivities to the input data at that time.\newline

Overall, the analyses highlight that the \acrshort{lstm}s generally offer more consistent and reliable performance, marked by less variability in spread and skewness compared to \acrshort{gru}s. While \acrshort{gru}s seemed to adjust its performance based on different conditions, suggesting a possible sensitivity to seasonal or environmental changes, the \acrshort{lstm}s maintained steadiness across the evaluated metrics. This consistent performance might render \acrshort{lstm}s more suitable for applications requiring consistency across varying conditions.

\subsection{Comparison of Final Lagrangian Visualisations}
\label{subsec:4.2.4}

The visual comparisons between Figures~\ref{fig_3.9} and~\ref{fig_4.8} clearly underscore notable differences in the outcomes of the \acrshort{lstm} and \acrshort{gru} simulations. Specifically, the August simulations reveal distinct disparities in the final particle locations between the \acrshort{lstm} and \acrshort{gru} models, whereas the November simulations display a high degree of similarity. This observation lends support to findings from the previous subsection, where the November results demonstrated greater alignment between the models compared to August. Such disparities underscore the unpredictable nature of these predictions and highlight the complex interplay of various factors that significantly impact the accuracy and consistency of the final outcomes. This variability is illustrative of the inherent challenges in modelling time series data, where slight variations in input or parameters can lead to markedly different predictions.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth,keepaspectratio]{LSMT_and_GRU_initial_vs_final_debris_movement_(November).pdf}
    \caption[LSTM and GRU initial vs final debris movement (November).]{-- \acrshort{lstm} and \acrshort{gru} initial vs final debris movement (November).\label{fig_4.8}}
\end{figure}

\vspace{-1cm}
\section{Summary}
\label{sec:4.3}

This chapter has meticulously explored the performance of \acrshort{lstm} and \acrshort{gru} models through a comprehensive evaluation strategy. The analysis was divided into two key areas: error metrics evaluation and geospatial analysis, which together provided a robust evaluation of the models' effectiveness and their practical applicability. 

The findings indicate that \acrshort{lstm} offers more consistent and reliable predictions, making it the preferable model when considering both components' performances under varied seasonal and environmental conditions. The geospatial analysis further corroborated these findings, showing that \acrshort{lstm} generally maintained more consistent performance metrics, such as spread and skewness, when compared to \acrshort{gru}. This consistency was evident despite the seasonal variations between the two dates, highlighting \acrshort{lstm}'s robustness across different predictive scenarios.

Contrary to our initial hypothesis posited in Sub-section ~\ref{subsec:4.2.1}, the analysis did not conclusively prove that proximity to the coast and reduced data availability significantly degraded model performance. While coastal data points generally showed less accuracy, this was not universally the case. Some coastal predictions maintained good accuracy, suggesting that other factors, such as the models' capacity to handle sparse data and environmental noise, play a critical role in prediction outcomes. Therefore, the hypothesis that less data inherently results in poorer model performance was not substantiated by the findings.
