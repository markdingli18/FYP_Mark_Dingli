\graphicspath{{content/chapters/4_evaluation/evaluation_figures}}

\chapter{Evaluation}
\label{chp:evaluation}

This chapter provides a detailed overview and a comprehensive explanation of the results obtained through the evaluation strategy outlined in Section~\ref{sec:3.5}. The primary objectives are to ascertain which model, \acrshort{lstm} or \acrshort{gru}, demonstrates superior performance, and to evaluate the similarity of the Lagrangian simulations generated using these models' predictions. As previously noted, the framework was executed on two specific dates—August 4th and November 4th, 2023—to gauge the models' consistency and reliability under varying seasonal conditions, offering a thorough analysis of their performance across different environmental dynamics. The chapter is structured into two sections: Section~\ref{sec:4.1} delves into the analysis of average error metrics to discern which model performed best. Section~\ref{sec:4.2} focuses on a geospatial analysis to investigate the impact of to find out if the locations and the amount of data play a role in the predictions, while also briefly comparing the merged predictions with the Lagrangian simulations. This approach ensures a thorough evaluation of the models' effectiveness and their applicability in real-world scenarios.

\section{LSTM vs GRU}
\label{sec:4.1}

In the initial experiment, we assessed the accuracy of the models in predicting sea surface current velocities by comparing the predicted results against actual historical values using three key error metrics:
\begin{itemize}
    \item \textbf{\acrshort{mae}:} This metric computes the average absolute difference between actual and predicted values across the dataset. It quantifies the typical magnitude of the prediction errors without considering their direction, providing a clear measure of average error.
    \item \textbf{\acrshort{mse}:} This represents the average of the squares of the differences between actual and predicted values. It accentuates larger errors more significantly than smaller ones by squaring the differences, highlighting impactful prediction discrepancies.
    \item \textbf{\acrshort{rmse}:} Calculated as the square root of the \acrshort{mse}, it measures the standard deviation of residuals, offering a scale-sensitive accuracy measure. It provides an indication of the typical magnitude of prediction errors in the same units as the data.
\end{itemize}

\subsection{Error Metrics Results}
\label{subsec:4.1.1}

The average error metrics for the 24-hour rolling predictions from all 37 models on the 4th of August are presented in the tables below:

\begin{table}[H]
    \caption{\acrshort{lstm} \textit{'u'} average error metrics (August).\label{tab:4.1}}
    \centering
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14126 & 0.22658 & 0.05830 \\
        \textbf{MSE} & 0.11693 & 0.51328 & 0.01012 \\
        \textbf{RMSE} & 0.17957 & 0.29100 & 0.05289 \\
        \hline
    \end{tblr}
\end{table}

\begin{table}[H]
    \caption{\acrshort{lstm} \textit{'v'} average error metrics (August).\label{tab:4.2}}
    \centering
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14370 & 0.13416 & 0.14072 \\
        \textbf{MSE} & 0.06405 & 0.10900 & 0.07281 \\
        \textbf{RMSE} & 0.18300 & 0.17483 & 0.21160 \\
        \hline
    \end{tblr}
\end{table}

\begin{table}[H]
    \caption{\acrshort{gru} \textit{'u'} average error metrics (August).\label{tab:4.3}}
    \centering
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14849 & 0.22171 & 0.06700 \\
        \textbf{MSE} & 0.11589 & 0.50339 & 0.01603 \\
        \textbf{RMSE} & 0.18693 & 0.28451 & 0.07046 \\
        \hline
    \end{tblr}
\end{table}

\begin{table}[H]
    \caption{\acrshort{gru} \textit{'v'} average error metrics (August).\label{tab:4.4}}
    \centering
    \begin{tblr}{|c|S[table-format=1.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 0.14472 & 0.13763 & 0.14908 \\
        \textbf{MSE} & 0.06558 & 0.11215 & 0.06614 \\
        \textbf{RMSE} & 0.18376 & 0.17936 & 0.20195 \\
        \hline
    \end{tblr}
\end{table}

The analysis of these results reveals insightful differences in model performance. For the \textit{'u'} component, \acrshort{lstm} models demonstrate slightly lower \acrshort{mae} and \acrshort{rmse}, indicating better average accuracy and consistency, although \acrshort{gru} models show a marginally lower \acrshort{mse}. Conversely, for the \textit{'v'} component, both models perform similarly with minimal variations across all metrics, which suggests a near-equivalent capability in handling this type of prediction. Further examination of the variability through Standard Deviation and \acrshort{iqr} metrics shows that \acrshort{lstm} models have a lower standard deviation in the \textit{'v'} component predictions, suggesting more consistent performance relative to \acrshort{gru}. Additionally, the smaller \acrshort{iqr} for \acrshort{lstm} in both components implies that its predictions are more tightly clustered around the median, indicating less variability and more reliability. While both models performed well, \acrshort{lstm} offered a marginally better performance, particularly for the \textit{'u'} component, establishing it as the preferable model. 

On the other hand, the results for the 24-hour rolling predictions for all 37 models on the 4th of November are detailed below:

\begin{table}[H]
    \caption{\acrshort{lstm} \textit{'u'} average error metrics (November).\label{tab:4.5}}
    \centering
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 1.03141 & 2.11801 & 0.29923 \\
        \textbf{MSE} & 14.76509 & 40.75769 & 0.24335 \\
        \textbf{RMSE} & 1.63416 & 3.47773 & 0.39745 \\
        \hline
    \end{tblr}
\end{table}

\begin{table}[H]
    \caption{\acrshort{lstm} \textit{'v'} average error metrics (November).\label{tab:4.6}}
    \centering
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 2.62164 & 5.50710 & 0.50617 \\
        \textbf{MSE} & 97.85786 & 253.42893 & 0.86051 \\
        \textbf{RMSE} & 4.23877 & 8.93816 & 0.84444 \\
        \hline
    \end{tblr}
\end{table}

\begin{table}[H]
    \caption{\acrshort{gru} \textit{'u'} average error metrics (November).\label{tab:4.7}}
    \centering
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 1.05110 & 2.11189 & 0.56820 \\
        \textbf{MSE} & 14.77234 & 40.77340 & 0.46635 \\
        \textbf{RMSE} & 1.65105 & 3.47079 & 0.58636 \\
        \hline
    \end{tblr}
\end{table}

\begin{table}[H]
    \caption{\acrshort{gru} \textit{'v'} average error metrics (November).\label{tab:4.8}}
    \centering
    \begin{tblr}{|c|S[table-format=2.5]|c|c|}
        \hline
        \textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{IQR} \\
        \hline
        \textbf{MAE} & 2.65316 & 5.50156 & 0.53727 \\
        \textbf{MSE} & 97.98028 & 253.62271 & 1.14379 \\
        \textbf{RMSE} & 4.26801 & 8.93109 & 0.99123 \\
        \hline
    \end{tblr}
\end{table}

The analysis reveals several insights. For the \textit{'u'} component, both models display relatively high \acrshort{mae}, \acrshort{mse}, and \acrshort{rmse}, with \acrshort{lstm} showing lower metrics. The high standard deviations observed for both models suggest a significant presence of outliers, indicating some predictions were inaccurate. This is evident in the \acrshort{gru} \textit{'u'} component where the \acrshort{iqr} is higher, suggesting a broader spread compared to \acrshort{lstm}, pointing to more frequent outlier performances in the \acrshort{gru} model. In contrast, the '\textit{v'} component shows considerably higher error values for both models, with \acrshort{gru} again having higher values across all metrics. The standard deviations and \acrshort{iqr} values are significantly larger in the \textit{'v'} component for both models, reinforcing the presence of outliers and indicating that predictions for the \textit{'v'} component were generally less accurate and more variable. Overall, the \acrshort{lstm} model performs slightly better than the GRU model, particularly in the \textit{'v'} component, as evidenced by the lower error metrics and narrower \acrshort{iqr}, which suggests better predictions. Therefore, the more consistent performance of LSTM across both components makes it the better model overall. 

\subsection{Error Metrics Discussion of Results}
\label{subsec:4.1.2}

The analysis highlights that predictions for the \textit{'u'} component (east-west velocity) were generally more accurate than for the \textit{'v'} component (north-south velocity). This discrepancy can be attributed to the alignment of radar systems, which are aligned in a north-south orientation, as depicted in Figure~\ref{sec:2.1}. This alignment potentially impacts the accuracy of \textit{'v'} component reading and inhibits the radar's ability to capture detailed north-south data, therefore leading to less accurate predictions for the \textit{'v'} component. 

Moreover, the first experiment conducted on the 4th of August exhibited notably better results—characterised by lower error values and fewer outliers—compared to the subsequent November evaluation. This improvement is likely due to the proximity of the August data to the final sequences of the test dataset, potentially leading to the models being better tuned to these conditions. Furthermore, the process of rolling forecasting, which bases predictions on preceding outputs, may lead to inaccuracies, particularly when initial predictions are derived from interpolated data. This method could inherently propagate errors, especially under conditions of notable missing data as discussed in Sub-section~\ref{subsec:3.3.3}.

Such findings underscore the necessity of considering temporal proximity and data integrity when assessing model performance. These phenomena and their implications on model performance will be explored further in the next section.

\section{Geospatial Analysis}
\label{sec:4.2}

In this section, we extended the analysis inspired by the insights garnered from the heat map depicted in Figure~\ref{sec:3.5}. Our objectives were multifaceted: we aimed to determine whether an increased volume of data correlates with enhanced predictive accuracy, whether data points closer to the coast—typically characterised by less data-yield poorer performance, and whether the time of year and seasonality of the data impact the results. The primary goal was to assess whether the geographical location of data points influences the accuracy of the predictive models. Additionally, we sought to compare the final Lagrangian simulations generated by both the \acrshort{lstm} and \acrshort{gru} models to evaluate their similarities in modelling sea surface debris.

\subsection{The Hypothesis}
\label{subsec:4.2.1}

As highlighted in Sub-sections~\ref{subsec:3.3.1} and~\ref{subsec:3.5.2}, our dataset contains a significant number of \acrshort{nan}s, with data points closer to the coast having more \acrshort{nan}s present, as evidenced in Figure~\ref{fig_3.5}. This is corroborated by Figure~\ref{fig_4.1}, which illustrates how some data points, have significantly less data available. Figure~\ref{fig_3.5} is instrumental in demonstrating the correlation between model performance and geographic location, thereby paving the way for a focused analysis on the impact of data availability at specific locations. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\textwidth,keepaspectratio]{histogram_of_data_points_by_location_number.pdf}
    \caption[Short sample caption.]{Histogram of Data Points by Location Number\label{fig_4.1}}
\end{figure}

Based on these observations, we formulated a hypothesis: \textit{A plausible hypothesis could propose that data points near the coast exhibit reduced data availability due to environmental and technical challenges that interfere with radar performance. This scarcity of data consequently impairs the accuracy of predictive models for sea surface current velocities near the coast. Specifically, it suggests that models predicting currents at coastal locations perform less effectively compared to those further offshore, where radar data tends to be more complete.}

The subsequent Subsections will analyse this hypothesis, employing a comparative analysis with the heat map presented in Figure~\ref{fig_3.5} to validate or refute our assumption regarding the influence of geographical location on model accuracy.

\subsection{Heat maps Results and Analysis}
\label{subsec:4.2.2}

As outlined in Sub-section~\ref{subsec:3.5.2}, we employed heat maps to analyse the performance of our models geospatially, focusing on the \acrshort{mae} across all 37 models. These visualisations are crucial for testing the validity of our hypothesis regarding the impact of data availability on predictive accuracy near coastal areas. Heat maps for the \acrshort{mae} error metrics were produced for both the \textit{'u'} and \textit{'v'} components of both \acrshort{lstm} and \acrshort{gru} models, covering both the August 4th and November 4th, 2023 predictions. To enhance the clarity of these visualisations and minimise the influence of outliers, we applied a clipping method at the 95th percentile of the data. This method effectively limited the range of data considered for colour scaling in the heat maps, allowing for more nuanced visual comparisons between most data points by excluding extreme outliers. The resultant heat maps for August 4th are displayed as follows:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.68\textwidth,keepaspectratio]{u_component_MAE_heatmaps_(Aug).pdf}
    \caption[Short sample caption.]{'u' component MAE heat maps (August).\label{fig_4.2}}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.68\textwidth,keepaspectratio]{v_component_MAE_heatmaps_(Aug).pdf}
    \caption[Short sample caption.]{'v' component MAE heat maps (August).\label{fig_4.3}}
\end{figure}
The heat maps for November 4th are detailed in the subsequent figures:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.68\textwidth,keepaspectratio]{u_component_MAE_heatmaps_(Nov).pdf}
    \caption[Short sample caption.]{'u' component MAE heat maps (November).\label{fig_4.4}}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.68\textwidth,keepaspectratio]{v_component_MAE_heatmaps_(Nov).pdf}
    \caption[Short sample caption.]{'v' component MAE heat maps (November).\label{fig_4.5}}
\end{figure}

The hypothesis regarding the impact of input data quality and quantity on model performance is nuanced by the evidence presented in the \acrshort{mae} heat maps. These visualisations clarify that while models near the coast generally perform worse, presumably due to less data being available, there are notable exceptions where coastal predictions maintained good accuracy. This observation challenges the assumption that greater data volume directly correlates with higher predictive accuracy. Instead, it suggests that the models' capabilities to handle noise and extract meaningful patterns from sparse data can significantly impact their effectiveness. Furthermore, the differential accuracy between the \textit{'u'} and \textit{'v'} components highlights the complexity of environmental factors and model sensitivities, which contribute to a diverse spectrum of outcomes that are not solely determined by the amount of data available.

Dividing the analysis between the August and November predictions reveals a stark contrast; predictions for August are markedly more accurate, underscoring the importance of the temporal proximity of training data to the predictive period and the impact of seasonal variations. The outliers observed on August 4th are not consistent with those on November 4th, highlighting the temporal variability of missing data and its non-uniform impact across different points and times. it becomes evident that no consistent pattern exists, thus challenging any definitive conclusions. While certain models excel in predicting the \textit{'u'} component, their performance diminishes when tasked with the \textit{'v'} component. These observations attest to the intricate dynamics at play in predictive modelling, where factors like the models' ability to manage noise and the inherent directional properties of data lead to outcomes where less data does not always result in less accuracy.

\subsection{Comparison of Lagrangian Simulations }
\label{subsec:4.2.3}

In the final component of the evaluation framework, we assessed the performance of the \acrshort{lstm} and \acrshort{gru} models by analysing their centroids, spreads, and skewness within their respective Lagrangian simulation outputs. The findings for August 4th are depicted in Figure~\ref{fig_4.6}:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth,keepaspectratio]{LSTM_vs_GRU_comparison_on_lagrangian_simulations_(august).pdf}
    \caption[Short sample caption.]{LSTM vs GRU Comparison on Lagrangian Simulations (August).\label{fig_4.6}}
\end{figure}
On the contrary, the results for the 4th of November are as follows:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth,keepaspectratio]{LSTM_vs_GRU_comparison_on_lagrangian_simulations_(november).pdf}
    \caption[Short sample caption.]{LSTM vs GRU Comparison on Lagrangian Simulations (August).\label{fig_4.7}}
\end{figure}