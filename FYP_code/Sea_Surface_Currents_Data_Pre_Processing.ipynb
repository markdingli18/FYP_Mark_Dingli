{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d53279",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a773b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc5b38",
   "metadata": {},
   "source": [
    "### Loading, Subsetting, and Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ace77e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2091/2091 [00:27<00:00, 77.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files loaded. Starting concatenation and interpolation...\n",
      "=============================================================================================================================\n",
      "Subsetting dataset for the specified polygon area...\n",
      "=============================================================================================================================\n",
      "Saving dataset to NetCDF file...\n",
      "=============================================================================================================================\n",
      "Dataset successfully saved.\n",
      "=============================================================================================================================\n",
      "Processing completed successfully.\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = \"Data/Sea Surface Currents\"\n",
    "OUTPUT_FILE = \"Data/Processed_SSC_Data.nc\"\n",
    "START_DATE = '2021-01-01'\n",
    "END_DATE = '2023-11-12'\n",
    "LON_MIN = 13.9\n",
    "LON_MAX = 14.81\n",
    "LAT_MIN = 35.6\n",
    "LAT_MAX = 36.3\n",
    "\n",
    "def load_and_select_ssc_data():\n",
    "    date_range = pd.date_range(START_DATE, END_DATE, freq='12H')\n",
    "    merged_dataset = []\n",
    "    time_list = []\n",
    "\n",
    "    for target_time in tqdm(date_range, desc=\"Processing files\"):\n",
    "        year = target_time.year\n",
    "        month = target_time.strftime('%m')\n",
    "        day = target_time.strftime('%d')\n",
    "        day_path = os.path.join(BASE_PATH, f\"SSC_MaltaSicily_{year}\", str(year), month, day)\n",
    "\n",
    "        if not os.path.exists(day_path):\n",
    "            continue\n",
    "\n",
    "        files = os.listdir(day_path)\n",
    "        nearest_file = min(files, key=lambda x: abs(pd.to_datetime(x.split('_')[-1][:4], format='%H%M') - target_time))\n",
    "        file_path = os.path.join(day_path, nearest_file)\n",
    "        with xr.open_dataset(file_path) as ds:\n",
    "            # Ensure time dimension matches target_time\n",
    "            ds = ds.assign_coords(time=[target_time])\n",
    "            merged_dataset.append(ds.copy())  # Copy the data to avoid issues after closing\n",
    "        time_list.append(target_time)\n",
    "\n",
    "    if merged_dataset:\n",
    "        combined_ds = xr.concat(merged_dataset, dim='time')\n",
    "        # Interpolate to fill missing time points\n",
    "        combined_ds = combined_ds.interp(time=date_range)\n",
    "        print(\"All files loaded. Starting concatenation and interpolation...\")\n",
    "        print(\"=\"*125)\n",
    "        return combined_ds\n",
    "    else:\n",
    "        print(\"No data loaded.\")\n",
    "        print(\"=\"*125)\n",
    "        return None\n",
    "\n",
    "def subset_data_for_polygon(dataset):\n",
    "    subset_ds = dataset.sel(lon=slice(LON_MIN, LON_MAX), lat=slice(LAT_MIN, LAT_MAX))\n",
    "    return subset_ds\n",
    "\n",
    "def save_to_netcdf(dataset):\n",
    "    print(\"Saving dataset to NetCDF file...\")\n",
    "    print(\"=\"*125)\n",
    "    dataset.to_netcdf(OUTPUT_FILE)\n",
    "    print(\"Dataset successfully saved.\")\n",
    "    print(\"=\"*125)\n",
    "\n",
    "# Main execution block\n",
    "combined_ds = load_and_select_ssc_data()\n",
    "\n",
    "if combined_ds is not None:\n",
    "    print(\"Subsetting dataset for the specified polygon area...\")\n",
    "    print(\"=\"*125)\n",
    "    subset_ds = subset_data_for_polygon(combined_ds)\n",
    "    save_to_netcdf(subset_ds)\n",
    "    print(\"Processing completed successfully.\")\n",
    "    print(\"=\"*125)\n",
    "else:\n",
    "    print(\"Data processing was unsuccessful. Please check your data files and paths.\")\n",
    "    print(\"=\"*125)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174fdae4",
   "metadata": {},
   "source": [
    "### Ensuring that the merged file is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f2765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Sea Surface Currents Dataset Information\n",
      "=============================================================================================================================\n",
      "\n",
      "Dataset Dimensions:\n",
      "FrozenMappingWarningOnValuesAccess({'lat': 25, 'lon': 22, 'time': 2091})\n",
      "\n",
      "Dataset Coordinates:\n",
      "Coordinates:\n",
      "  * lat      (lat) float32 35.74 35.77 35.79 35.81 ... 36.21 36.23 36.26 36.28\n",
      "  * lon      (lon) float32 13.92 13.96 14.0 14.04 ... 14.65 14.69 14.73 14.77\n",
      "  * time     (time) datetime64[ns] 2021-01-01 2021-01-01T12:00:00 ... 2023-11-12\n",
      "\n",
      "Data Variables in the Dataset:\n",
      "Data variables:\n",
      "    u        (time, lat, lon) float64 ...\n",
      "    v        (time, lat, lon) float64 ...\n",
      "    stdu     (time, lat, lon) float64 ...\n",
      "    stdv     (time, lat, lon) float64 ...\n",
      "    cov      (time, lat, lon) float64 ...\n",
      "    velo     (time, lat, lon) float64 ...\n",
      "    head     (time, lat, lon) float64 ...\n",
      "\n",
      "Attributes (Metadata) in the Dataset:\n",
      "{'NC_GLOBAL.Title': 'Near-Real time Surface Ocean Velocity', 'NC_GLOBAL.origin': 'BARK (measured);POZZ (measured);', 'NC_GLOBAL.source': 'HF Radar Derived Surface Currents obtained from CODAR combine method', 'NC_GLOBAL.history': '08-Jun-2023 14:46:23', 'NC_GLOBAL.grid_type': 'REGULAR', 'NC_GLOBAL.Conventions': 'CF-1.4', 'NC_GLOBAL.creator_url': 'http://www.qualitasinstruments.com/', 'NC_GLOBAL.institution': 'Qualitas Remos', 'NC_GLOBAL.creator_name': 'Macu Ferrer', 'NC_GLOBAL.creator_email': 'macu.ferrer@qualitasremos.com', 'NC_GLOBAL.grid_projection': 'equidistal cylindrical', 'NC_GLOBAL.grid_resolution': '3.0km', 'NC_GLOBAL.geospatial_lat_max': 36.8802, 'NC_GLOBAL.geospatial_lat_min': 35.7447, 'NC_GLOBAL.geospatial_lon_max': 15.3804, 'NC_GLOBAL.geospatial_lon_min': 13.6768, 'NC_GLOBAL.netcdf_library_version': 'v2'}\n",
      "\n",
      "Expected Time Points: 2091\n",
      "Actual Time Points: 2091\n",
      "\n",
      "Latitude Range in the Dataset: 35.74470138549805 to 36.27909851074219\n",
      "Longitude Range in the Dataset: 13.92020034790039 to 14.772000312805176\n",
      "\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Open the merged dataset\n",
    "ds = xr.open_dataset(OUTPUT_FILE)\n",
    "\n",
    "# Calculate the expected number of time points\n",
    "date_range = pd.date_range(start=START_DATE, end=END_DATE, freq='12H')\n",
    "expected_time_points = len(date_range)\n",
    "\n",
    "# Print section separator\n",
    "print(\"=\" * 125)\n",
    "print(\"Sea Surface Currents Dataset Information\")\n",
    "print(\"=\" * 125)\n",
    "\n",
    "# Now, print the structure and check the dataset\n",
    "print(\"\\nDataset Dimensions:\")\n",
    "print(ds.dims)\n",
    "print(\"\\nDataset Coordinates:\")\n",
    "print(ds.coords)\n",
    "print(\"\\nData Variables in the Dataset:\")\n",
    "print(ds.data_vars)\n",
    "print(\"\\nAttributes (Metadata) in the Dataset:\")\n",
    "print(ds.attrs)\n",
    "\n",
    "# Check that the time dimension is as expected\n",
    "actual_time_points = ds.sizes['time']\n",
    "print(\"\\nExpected Time Points:\", expected_time_points)\n",
    "print(\"Actual Time Points:\", actual_time_points)\n",
    "\n",
    "# Check that the lat/lon are within the specified bounds\n",
    "lat_min, lat_max = ds['lat'].min().values, ds['lat'].max().values\n",
    "lon_min, lon_max = ds['lon'].min().values, ds['lon'].max().values\n",
    "print(\"\\nLatitude Range in the Dataset:\", lat_min, \"to\", lat_max)\n",
    "print(\"Longitude Range in the Dataset:\", lon_min, \"to\", lon_max)\n",
    "\n",
    "# Check for expected variables\n",
    "assert 'u' in ds.variables, \"u variable is missing from the dataset\"\n",
    "assert 'v' in ds.variables, \"v variable is missing from the dataset\"\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 125)\n",
    "\n",
    "# Close the dataset after inspection\n",
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
