{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Model for Predicting Sea Surface Currents and Wind Conditions\n",
    "\n",
    "This notebook outlines the steps to develop a CNN-LSTM hybrid model for predicting future sea surface currents and wind conditions. We'll go through data preprocessing, model definition, training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\markd\\anaconda3\\envs\\oceanparcels\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Dense\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_ds = xr.open_dataset('Data/Processed_SSC_Data.nc', engine='h5netcdf')\n",
    "wind_ds = xr.open_dataset('Data/Processed_Wind_Data.nc', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_normalize = ['u', 'v', 'u10', 'v10'] \n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normalize variables in the SSC dataset\n",
    "ssc_variables_to_normalize = ['u', 'v']  \n",
    "for var in ssc_variables_to_normalize:\n",
    "    data = ssc_ds[var].values.flatten()\n",
    "    scaled_data = scaler.fit_transform(data.reshape(-1, 1)).reshape(ssc_ds[var].shape)\n",
    "    ssc_ds[var].values = scaled_data\n",
    "\n",
    "# Normalize variables in the Wind dataset\n",
    "wind_variables_to_normalize = ['u10', 'v10']  \n",
    "for var in wind_variables_to_normalize:\n",
    "    data = wind_ds[var].values.flatten()\n",
    "    scaled_data = scaler.fit_transform(data.reshape(-1, 1)).reshape(wind_ds[var].shape)\n",
    "    wind_ds[var].values = scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating Wind Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate wind data to match the SSC dataset's spatial grid\n",
    "latitude_new = ssc_ds['lat']  # Latitudes from the SSC dataset\n",
    "longitude_new = ssc_ds['lon']  # Longitudes from the SSC dataset\n",
    "\n",
    "wind_ds_interpolated = wind_ds.interp(latitude=latitude_new, longitude=longitude_new, method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine SSC and Wind Data for Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150050, 2) (1150050, 2)\n"
     ]
    }
   ],
   "source": [
    "# Correct approach to flatten and combine after ensuring matching spatial dimensions\n",
    "ssc_flat = np.stack([ssc_ds['u'].values.flatten(), ssc_ds['v'].values.flatten()], axis=-1)\n",
    "wind_flat = np.stack([wind_ds_interpolated['u10'].values.flatten(), wind_ds_interpolated['v10'].values.flatten()], axis=-1)\n",
    "\n",
    "# Ensure this by verifying their shapes\n",
    "print(ssc_flat.shape, wind_flat.shape)\n",
    "\n",
    "# If the shapes match, proceed to concatenate\n",
    "combined_data = np.concatenate([ssc_flat, wind_flat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Creation for Model Input\n",
    "Here we convert the dataset into sequences that the model can use for training. Each sequence contains data from a series of consecutive time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, sequence_length=30):\n",
    "    sequences = []\n",
    "    output_data = []\n",
    "    for i in range(len(input_data) - sequence_length):\n",
    "        sequences.append(input_data[i:i+sequence_length])\n",
    "        output_data.append(input_data[i+sequence_length])\n",
    "    return np.array(sequences), np.array(output_data)\n",
    "\n",
    "sequence_length = 30  # Number of time steps for each input sequence\n",
    "\n",
    "# Assuming combined_data is shaped correctly and represents your entire dataset's flattened spatial points across all time steps\n",
    "X, y = create_sequences(combined_data, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_train shape: (920016, 30, 4)\n",
      "Original X_val shape: (230004, 30, 4)\n",
      "Total elements in X_train: 110401920\n",
      "Total elements in X_val: 27600480\n",
      "Expected number of elements after reshape: 240\n"
     ]
    }
   ],
   "source": [
    "# Define width and channels\n",
    "width = 2  # Number of spatial dimensions (latitude and longitude)\n",
    "channels = 4  # Number of variables (u, v, u10, v10)\n",
    "\n",
    "# Split the sequences into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Original X_train shape: {X_train.shape}\")\n",
    "print(f\"Original X_val shape: {X_val.shape}\")\n",
    "total_elements_train = np.prod(X_train.shape)\n",
    "total_elements_val = np.prod(X_val.shape)\n",
    "print(f\"Total elements in X_train: {total_elements_train}\")\n",
    "print(f\"Total elements in X_val: {total_elements_val}\")\n",
    "\n",
    "expected_elements = sequence_length * width * channels\n",
    "print(f\"Expected number of elements after reshape: {expected_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(sequence_length, width, channels):\n",
    "    model = Sequential([\n",
    "        TimeDistributed(Conv1D(64, 3, activation='relu'), input_shape=(sequence_length, width, channels)),\n",
    "        TimeDistributed(MaxPooling1D(2)),\n",
    "        TimeDistributed(Flatten()),\n",
    "        LSTM(50, activation='relu'),\n",
    "        Dense(4, activation='linear')  # Output features: u, v, u10, v10\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 27600480 into shape (30,920016,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m width \u001b[38;5;241m=\u001b[39m total_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (sequence_length \u001b[38;5;241m*\u001b[39m channels)\n\u001b[0;32m      7\u001b[0m X_train_reshaped \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, sequence_length, width, channels))\n\u001b[1;32m----> 8\u001b[0m X_val_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mX_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Build the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model(sequence_length, width, channels)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 27600480 into shape (30,920016,4)"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "width = 22 * 25  # Total spatial points when flattened\n",
    "channels = 4  # Features per spatial point\n",
    "\n",
    "X_train_reshaped = X_train.reshape((-1, sequence_length, width, channels))\n",
    "X_val_reshaped = X_val.reshape((-1, sequence_length, width, channels))\n",
    "\n",
    "# Build the model\n",
    "model = build_model(sequence_length, width, channels)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_reshaped, y_train, \n",
    "    epochs=20, \n",
    "    validation_data=(X_val_reshaped, y_val),\n",
    "    verbose=0,  # Turn off the default verbose output\n",
    "    callbacks=[TqdmCallback(verbose=2)]  # Adjust verbosity for TqdmCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = model.evaluate(X_val_reshaped, y_val)\n",
    "print(f'Validation loss: {val_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oceanparcels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
